As mentioned in the Introduction \todo{Check where that was} the majority of
functional languages use either call-by-need or call-by-value semantics.  While
call-by-need has many attractive properties the delaying of computation incurs
an overhead cost on all computations. Call-by-value, in contrast, has an
execution model that is more easily mapped to conventional hardware, allowing
for simpler implementations that achieved good performance \tocite{SPJ or
someone}. Mycroft used this tension to motivate his development of strictness
analysis \tocite{mycroft thesis}:

\begin{displayquote}

The above arguments suggest that call-by-value is more efficient but
call-by-need preferable on aesthetic/definedness considerations. So techniques
are herein developed which allow the system to present a call-by-need interface
to the user but which performs a pre-pass on his program annotating those
arguments which can validly be passed using call-by-value.

\end{displayquote}

By determining which arguments can be safely passed using call-by-value
we diminish the overhead of call-by-need, paying the overhead of
suspending computation only when necessary to ensure that call-by-need
semantics are maintained.

While this was the original motivation for strictness analysis it also serves
in identifying potential parallelism in a program. When an argument is suitable
to be passed as call-by-value it is also suitable to be evaluated in parallel.
For this use the suspension becomes a future \tocite{futures}, the value is
evaluated in parallel to the original thread, synchronisation is then
accomplished via the same mechanism as laziness except a thread can be blocked
while waiting for another thread to complete its evaluation.

\subsection{Safety First}

Strictness analysis is chiefly concerned with \emph{safety}. In order to retain
the origin call-by-need semantics the runtime can only alter the evaluation order
when doing so guarantees the same termination properties of the program.

We will refer to this notion of safety as the \emph{strictness} properties
of an argument. Take a function $f$ of $n$ arguments

\begin{equation*}
f \ x_{1} \ \dots x_{i} \ \dots \ x_{n} \ = \langle \texttt{function body} \rangle
\end{equation*}

The $i$th argument of $f$ is said to be strict \emph{if and only if}

\begin{equation}
f \ x_{1} \ \dots \bot_{i} \ \dots \ x_{n} \ =  \bot
\end{equation}
\label{eq:idealSafety}

For any possible values of $x_{1}-x_{i-1},x_{i+1}-x_{n}$\footnote{Throughout this
dissertation $\bot$ is used to represent all forms of non-termination.}.

Equation \ref{eq:idealSafety} can be read as ``$f$ is strict in $x_{i}$ when $f$
fails to terminate if $x_{i}$ fails to terminate''. The reason this allows us
to evaluate the $i$th argument before its needed is because doing so would only result
in introducing non-termination if the program would have resulted in non-termination
otherwise.

\subsection{Abstract Domains}

Now that we have established what it means to be strict we can expand on how we
analyse programs for this property. As with any abstract interpretation
\todo{ensure this term is defined} this involves the choice of an abstract domain.

In non-strict languages types like integers and Booleans form a flat domain;
either we have a value of that type, or we have $\bot$. This is depicted in
Figure \ref{fig:flatInts}. We can form an intuition of these orderings by
thinking about how much we \emph{know} about a certain value. While the
interger value $5$ maybe more greater than the integer value $4$, we know the
same amount about each of them, their values. However, if we have a procedure
that is meant to compute an integer, and therefore has the type \<Int\>, and it
loops forever, we cannot know know that integer's value. Therefore we know less
about a non-terminating value.

%% Hasse diagram for the flat domain of integers.
%%
%% We have a horizontal 'number line' from -infity to infinity and edges from each
%% number to _|_ which is centered below the numbers.
\begin{figure}[h]
\centering
\begin{tikzpicture}
    \node [hasse, label=above:\large$0$]                (zero) at (0,0) {};
    \node [hasse, right = of zero, label=above:\large$1$] (one)   {};
    \node [hasse, right = of one, label=above:\large$2$]  (two)   {};
    \node [hasse, left = of zero, label=above:\large$-1$]  (neg1)  {};
    \node [hasse, left = of neg1, label=above:\large$-2$]  (neg2)  {};
    \node [hasse, left = of neg2, label=above:\large$\dots$]  (ldots) {};
    \node [hasse, right = of two, label=above:\large$\dots$]  (rdots) {};
    \node [hasse, left = of ldots, label=above:\large$-\infty$] (nInf)  {};
    \node [hasse, right = of rdots, label=above:\large$\infty$](inf)   {};

    \node [hasse, below = of zero, label=below:\large$\bot$] (bot)   {};

    \draw[black] (zero) -- (bot);
    \draw[black] (one) -- (bot);
    \draw[black] (two) -- (bot);
    \draw[black] (neg1) -- (bot);
    \draw[black] (neg2) -- (bot);
    \draw[black] (ldots) -- (bot);
    \draw[black] (rdots) -- (bot);
    \draw[black] (inf) -- (bot);
    \draw[black] (nInf) -- (bot);
\end{tikzpicture}
\caption{Flat Domain}
\label{fig:flatInts}
\end{figure}

This fits nicely with call-by-need semantics: an argument to a function of
type \<Int\> is really a computation that can either result in a value, or
result in non-termination. In terms of strictness analysis, this allows us to
abstract our real domain of integers to the simple two-point domain shown in
figure \ref{fig:twoPointNice}.

This is the domain we use for basic strictness analysis. The bottom of the
lattice, $\bot$, as implied above, represents \emph{definitely non-terminating}
expressions. The top of the lattice, $\top$, is used to represent
\emph{potentially terminating}\footnote{Remember that program analysis must
approximate in the general case.} expressions. This approximation is can seem
counterintuitive; why are we allowing the analysis to say some results are
potentially terminating when they could be non-terminating? The reasoning is
that non-termination is okay under non-strict semantics! If we approximated in
the opposite direction (as analyses for other purposes sometimes do) we may
accidentally compute a value that was never needed, defeating the purpose of
call-by-need evaluation.


\begin{figure}
\centering
\begin{tikzpicture}
    \node [hasse, label=above:\large$\top$]                 (top) {};
    \node [hasse, below = of top, label=below:\large$\bot$] (bot) {};

    \draw[black] (top) -- (bot);
\end{tikzpicture}
\caption{Two-point Domain}
\label{fig:twoPointNice}
\end{figure}

\subsubsection{Abstracting Functions}

Now that we know what it means to be strict and why we represent flat domains
as a two-point domain the next step is to abstract the functions in our
program. 

The idea is simple: for every function in our program of type \<A\> we must
produce a function of type \<A^{\#}\> that works on the abstracted
values\footnote{Some texts represent an abstracted type by a number \<N\> where
$N$ is the number of points in the abstract domain. We prefer to retain the
context of where this abstract domain came from.}. This abstracted program
is then interpreted using an abstract semantics that provides us with the
strictness information for each function in our program.

\todoinline{standard abstract interpretation diagram}


While this separation of abstracting the program and then performing an
abstract interpretation is useful from the theoretical point of view, many
compilers skip the intermediate representation of an abstracted program and
perform the abstract interpretation with the original AST
\citep{hinze1995projection, kubiak, SergeyDemand}.

We begin with the set of primitive arithmetic functions. In the case of F-Lite,
each numeric primitive is strict in both arguments, providing us with the
following for each of \<(+), (-), (*), (/)\>:

\begin{haskell*}
(\(\odot\)) &::& Int^{\#} -> Int^{\#} -> Int^{\#} \\
\(\top\) \(\odot\) \(\top\) &=& \(\top\) \\
\(\top\) \(\odot\) \(\bot\) &=& \(\bot\) \\
\(\bot\) \(\odot\) \(\top\) &=& \(\bot\) \\
\(\bot\) \(\odot\) \(\bot\) &=& \(\bot\)
\end{haskell*}

We must also be able combine results from different paths in a program. This
requires both conjunction and disjunction. We can use the \<meet\> ($\sqcap$)
and \<join\> $\sqcup$ from our lattice which are fully described in Figure
\ref{fig:twopointMeet}.

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
\begin{haskell*}
\(\top\) \(\sqcap\) \(\top\) &=& \(\top\) \\
\(\top\) \(\sqcap\) \(\bot\) &=& \(\bot\) \\
\(\bot\) \(\sqcap\) \(\top\) &=& \(\bot\) \\
\(\bot\) \(\sqcap\) \(\bot\) &=& \(\bot\)
\end{haskell*}
\end{minipage}
\quad\quad
\begin{minipage}{.5\textwidth}
\begin{haskell*}
\(\top\) \(\sqcup\) \(\top\) &=& \(\top\) \\
\(\top\) \(\sqcup\) \(\bot\) &=& \(\top\) \\
\(\bot\) \(\sqcup\) \(\top\) &=& \(\top\) \\
\(\bot\) \(\sqcup\) \(\bot\) &=& \(\bot\)
\end{haskell*}
\end{minipage}
\caption{The \emph{meet} ($\sqcap$) and \emph{join} ($\sqcup$) for our lattice}
\label{fig:twopointMeet}
\end{figure}

We can now define an abstract interpretation, $\mathcal{A}$, that takes
expressions in our language and gives us their abstracted values.

\begin{figure}
\begin{haskell*}
\mathcal{A} &::& Exp \to Env^{\#} \to Exp^{\#} \\
\mathcal{A}\Sem{Var v} \hasphi &=& \hasphi v \\
\mathcal{A}\Sem{Int i} \hasphi &=& \(\top\) \\
\mathcal{A}\Sem{Con c} \hasphi &=& \(\top\) \\
\mathcal{A}\Sem{App (Fun f) as} \hasphi &=& f^{\#} (map \mathcal{A} as) \\
\mathcal{A}\Sem{Let bs e}       \hasphi &=& \mathcal{A}\Sem{e} \\
\mathcal{A}\Sem{Case e alts}    \hasphi &=& \mathcal{A}\Sem{e} \(\sqcap\) \mathcal{C}\Sem{alts} \hasphi \\
\quad&\quad&\quad \\
\mathcal{C}\Sem{alt_{1}, alt_{2}, \(\dots\), alt_{n}} \hasphi &=& \mathcal{A}\Sem{alt_{1}} \hasphi \(\sqcup\) \mathcal{A}\Sem{alt_{2}} \hasphi \(\sqcup \dots\) \(\sqcup\) \mathcal{A}\Sem{alt_{n}} \hasphi
\end{haskell*}
\end{figure}

\todoinline{Fix rule C so that we take into account the vars introduced by case
alternatives (they should all be approximated as Top)}



%
%data Exp &=& App Exp [Exp] \\ &|& Case Exp [Alt] \\ &|& Let [Binding] Exp \\
%&|& Fun Id \\
