Because we are working in a lazy language it is not always safe to evaluate the
arguments to a function before we enter the body of a function. This is easy
to see with a simple example; appending two lists\footnote{Here we have used
the na\"{i}ve recursive version, but any correct version of append will
have the same strictness properties.}:

\begin{verbatim}
    append :: [a] -> [a] -> [a]
    append []     ys = ys
    append (x:xs) ys = x : append xs ys
\end{verbatim}

\verb|append| takes two list arguments, a central question that strictness
analysis asks is: How \emph{defined} must the arguments be to \verb|append|
in order for \verb|append| to terminate?

The first hint is that \verb|append| pattern matches on its first argument.
Because the function must be able to distinguish between a \verb|(:)| and a
\verb|[]| we know that the first argument must be defined \emph{at least} to
the outermost constructor. Therefore a use of \verb|append| that is passed
$\bot$ as its first argument will result in non-termination. What about the
second argument, \verb|ys|? Determining how defined \verb|ys| must be turns
out to be impossible without taking into account the \emph{context} that
a call to \verb|append| occurs in. If the first argument is \verb|[]|
then \verb|ys| must be defined to WHNF. However the \verb|(:)| case
guards the recursive call to \verb|append| and, by extension, the use
of \verb|ys|.

The literature on Strictness Analysis is the story of determining these
properties in the general case. We start in \S \ref{sec:idealAnalysis} with
exploring the simplest Strictness Analysis, \emph{Ideal Analysis} on \emph{flat
domains}. We then show how the work was extended to \emph{non-flat domains} in
\S \ref{sec:4pointDomains}. Lastly, we show how the notion of \emph{contexts}
mentioned above are formalised by the use of \emph{projections} from Domain
Theory in section \ref{sec:projections}.


\subsection{Ideal Strictness Analysis}
\label{sec:idealAnalysis}

However, if a function uses the value of an argument within its body it is safe
to evaluate that argument before, or in parallel to, the execution of the body
of the function. In order to determine which arguments can be evaluated in this
way modern compilers use \emph{strictness analysis} \citep{mycroft1980theory}.
More formally, a function $f$ of $n$ arguments

$$
    f\ x_{1} \dots \ x_{i} \ \dots x_{n} = \dots
$$

\noindent is strict in its $i$th argument if and only if

$$
    f\ x_{1} \dots \ \bot \ \dots x_{n} = \bot
$$

What this states is that $f$ is only strict in its $i$th argument if $f$
becomes non-terminating\footnote{In this paper we use the convention that
$\bot$ represents erroneous or non-terminating expressions.} by passing a
non-terminating value as its $i$th argument.

Knowing the strictness information of a function is the first step in automatic
parallelisation. This is because if $f$ is strict in its $i$th argument we do
not risk introducing non-termination (which would not otherwise be present) by
evaluating the $i$th argument in parallel. In other words, evaluating $x_{i}$ in
parallel would only introduce non-termination to the program if evaluating $f$
with $x_{i}$ would have resulted in $f$'s non-termination anyway.

F-Lite has two primitives for taking advantage of strictness information: $par$
and $seq$.
\begin{figure}
\begin{align*}
    &seq \ :: \ a \rightarrow b \rightarrow b &&par \ :: \ a \rightarrow b \rightarrow b \\
    &seq \ x \ y = y                          &&par \ x \ y = y
\end{align*}
\caption{Semantics of \texttt{seq} and \texttt{par}.}
\label{fig:seqandpar}
\end{figure}

Both functions return the value of their second argument. The difference is in
their \emph{side-effects}. $seq$ returns its second argument only \emph{after}
the evaluation of its first argument. $par$ forks the evaluation of its first
argument in a new parallel thread and then returns its second argument; this is
known as \emph{sparking} a parallel task \citep{clack1986four}.

Strictness analysis was a very active research area in the 1980's and the
development of analyses that provide the type of strictness information
outlined above is a well understood problem \citep{mycroft1980theory,
clack1985strictness, burn1986strictness}.  However, as outlined above,
strictness analysis does not provide satisfactory information about complex
data-structures \citep{wadler1987strictness}. This can be remedied by the
use of \emph{projections} to represent \emph{demand}.

\subsection{Abstract Interpretation}

Mycroft introduced the use of abstract interpretation for performing strictness
analysis on call-by-need programs over thirty years ago
\citep{mycroft1980theory}.
Strictness analysis as originally described by Mycroft was only capable of
dealing with a two-point domain (values that are definitely needed, and values
that may or may not be needed). This works well for types that can be
represented by a flat domain (Integer, Char, Bool, etc.)\footnote{Any type that
can be represented as an enumerated type.} but falls short on more complex data
structures. For example, even if we find that a function is strict in a list
argument, we can only evaluate up to the first \verb'cons' safely. For many
functions on lists, evaluating the entire list, or the spine of the list, is
safe; canonical examples are \verb'sum' and \verb'length'.

In order to accommodate this type of reasoning, Wadler developed a
\emph{four-point domain} for the abstract interpretation of list-processing
programs \citep{wadler1987strictness}. However, when extended in the natural way
for general recursive data structures, the size of the domains made finding
fix-points prohibitively costly.

\section{Projections and Contexts}
\label{sec:projections}

So far our discussion of strictness has only involved two levels of
`definedness': a defined value, or $\bot$. This is the whole story when dealing
with \emph{flat} data-structures such as Integers, Booleans or Enumerations.
However, in lazy languages nested data-structures have \emph{degrees} of
definedness.

Take the following example function and value definitions in F-Lite

\begin{centering}
\begin{BVerbatim}
length []     = 0                   sum []     = 0
length (x:xs) = 1 + length xs       sum (x:xs) = x + sum xs

definedList = [1,2,3,4]             infiniteList = [1,2,3...

partialList = [1,2,bot,4]           loop = loop
\end{BVerbatim}
\end{centering}\\

Both \verb-length- and \verb-sum- are functions on lists, but they use lists
differently. \verb-length- does not use the elements of its argument list.
Therefore \verb-length- would accept \verb-definedList- and \verb-partialList-
(which has a non-terminating element) as arguments and still return the correct
value. On the other hand \verb-sum- \emph{needs} the elements of the list,
otherwise it would not be able to compute the sum. For this reason, \verb-sum-
only terminates if it is passed a fully defined list and would result in
non-termination if passed \verb-partialList-. Neither function would terminate
if passed \verb-infiniteList-, since even \verb-length- requires the list to
have a finite length (some functions do not require a finite list, such as
\verb-head-, the function that returns the first element in a list). With
these examples we say that \verb-length- \emph{demands} a finite list, whereas
\verb-sum- \emph{demands} a fully-defined list.

This additional information about a data-structure is extremely useful when
trying to parallelise programs. If we can determine \emph{how much} of a
structure is needed we can then evaluate the structure to that depth in
parallel.

%The first notable attempt at capturing this type of information was
%by Wadler in 1987 \citep{wadler1987strictness}. However, his approach worked
%well on lists but did not scale well to other, more complex data-structures.

The work that introduced this representation of demands was by Wadler and
Hughes \citep{wadler1987projections} using the idea of \emph{projections} from
domain theory.  The technique we use in our compiler is a projection-based
strictness analysis based on the work in Hinze's dissertation
\citep{hinze1995projection}.  Hinze's dissertation is also a good resource for
learning the theory of projection-based strictness analysis.


\subsection*{Strategies}

With the more sophisticated information provided by projection-based analysis,
we require more than simply $par$ and $seq$. To this end we use the popular
technique of \emph{strategies} for parallel evaluation \citep{strategies,
marlow2010seq}. Strategies are designed to evaluate structures up to a certain
depth in parallel to the use of those structures. Normally, strategies are
written by the programmer for use in hand-parallelised code. In order to
facilitate auto-parallelisation we have developed a method to \emph{derive} an
appropriate strategy from the information provided to us by projection-based
strictness analysis. The rules for the derivation are presented as a
denotational semantics and can be found in our earlier work \citep{calderon}.

\section{The Granularity Problem}

We have now discussed how we find the parallelism that is implicit in our
program, but none of the analysis we provide determines whether the safe
parallelism is \emph{worthwhile}. Often static analysis will determine that a
certain structure is \emph{safe} to compute in parallel, but it is very
difficult to know when it is actually of any benefit. Parallelism has overheads
that require the parallel tasks to be substantial enough to make up for the
cost. A \emph{fine-grained} task is unlikely to require more computation than
the cost of sparking and managing the thread, let alone the potential to
interrupt productive threads \citep{hammond2000research, hogen1992automatic}.

One of the central arguments in our work is that static analysis \emph{alone}
is insufficient at finding both the implicit parallelism and determining whether
the introduced parallelism is substantial enough to warrant the overheads.

Our proposal is that the compiler should \emph{run} the program and use the
information gained from running it (even if it only looks at overall execution
time) to \emph{remove} the parallelism that is too fine-grained. By doing this
we shift the burden of the granularity problem away from our static analysis
and onto our search techniques. This way our static analysis is only used to
determine the safe parallel expressions, and not the granularity of the
expressions.

Here we will describe the method by which we identify the \emph{safe}
parallelism in F-Lite programs and arrange for the evaluation of these
expressions in parallel. The \emph{strictness} properties of a function
determine which arguments are definitely needed for the function to terminate,
whereas the \emph{demand} on an argument tells us \emph{how much} of the
argument's structure is needed. \emph{Strategies} are functions that evaluate
their argument's structure to a specific depth. By analysing the program for
strictness and demand information, we can then generate strategies for the
strict arguments to a function and evaluate the strategies in parallel to the
body of the function. The strategies we generate will only evaluate the
arguments to the depth determined by the demand analysis.
