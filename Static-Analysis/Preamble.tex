Non-strictness makes it difficult to reason about when expressions are
evaluated. This is due to the fact that call-by-need languages only evaluate
expressions when their results are needed. One of the benefits of this
approach is that it forces the programmer to avoid the use of arbitrary
side-effects. The resulting purity means that functions in pure functional
languages are \emph{referentially transparent}, or the result of a function
depends only on the values of its arguments (i.e.  there is no global state
that could effect the result of the function or be manipulated by the
function).

Unfortunately this apparently sensible evaluation model is actually at odds
with the goals of performance through parallelism: if we have parallel
processing resources, we wish to use them to do as much work as possible to
shorten execution time \citep{tremblay1995impact}.

Call-by-need semantics forces the compiler to take care in deciding which
sub-expressions can safely be executed in parallel.  Having a simple
parallelisation heuristic such as `compute all arguments to functions in
parallel' can alter the semantics of a non-strict language, introducing
non-termination or runtime errors that would not have occurred during a
sequential execution.

The process of determining which arguments are required for a function is known
as \emph{strictness analysis} \citep{mycroft1980theory}. Since the early 1980's
such analysis has been widely used for reducing the overheads of lazy evaluation
\citep{SergeyDemand}. As strictness analysis is a form of termination analysis
it is undecidable in general and therefore any results are approximate. Usually
the abstract semantics are chosen so that the analysis can determine when
an expression is \emph{definitely} needed.

It is possible to throw caution to the wind and \emph{speculate} on which
expressions may be needed. This itself is a rich area of research and requires
the compiler to identify plausible candidates but ensure that errors and
non-termination do not affect the program as a whole \tocite{speculative 
parallelism work}. For our work we chose to utilise only \emph{conservative}
implicit parallelism.

\defineword{Conservative Implicit Parallelism}{The parallelism inherent in
a program from tasks/expressions that would have been evaluated during
the program's sequential execution.}

There are downsides to the conservative approach which we will discuss in
\todo{where will I discuss this?}. However, conservative parallelism is more in
line with our overall goal of providing a system that guarantees that the
parallelised program has the same semantics as the original. Therefore, before
we can run our automatically parallelised programs, we must develop methods and
techniques for the compiler to \emph{find} and \emph{express} the parallelism
that is implicit in our programs. Strictness analysis is suitable in aiding
this task but care must be taken in choosing a specific analysis to use.

This chapter is concerned with studying the alternative analyses and the
trade-offs that are inherent in the differing approaches. A survey of the
concepts and development of strictness analysis will inform our choice of
analysis and allow us to understand the drawbacks and limits of our chosen
method.

\subsection*{Plan of the Chapter}

We provide a high-level overview of the issues and motivations in Section
\ref{sec:strictnessOverview}, this should provide enough context to those who
want to move quickly to the next chapter and not concern themselves with the
details of strictness analysis. The ideas are then expanded in the three
sections that follow. Section \ref{sec:twoPoint} explores basic strictness
analysis using a two-point domain. We will see why a two-point domain results
in an analysis that is too limited for our use in deriving useful parallel
strategies. Using a four-point domain, which we discuss in Section
\ref{sec:fourPoint}, fixes much of this issue and provides much better
information for parallel programs (and has been used towards that end) but does
not allow for analysis on arbitrary data-types. Lastly we review the work on
projection based analysis, which solves both issues, in Section
\ref{sec:projections}.
