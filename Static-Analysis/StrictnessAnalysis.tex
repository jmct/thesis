Here we will describe the method by which we identify the \emph{safe}
parallelism in F-Lite programs and arrange for the evaluation of these
expressions in parallel. The \emph{strictness} properties of a function
determine which arguments are definitely needed for the function to terminate,
whereas the \emph{demand} on an argument tells us \emph{how much} of the
argument's structure is needed. \emph{Strategies} are functions that evaluate
their argument's structure to a specific depth. By analysing the program for
strictness and demand information, we can then generate strategies for the
strict arguments to a function and evaluate the strategies in parallel to the
body of the function. The strategies we generate will only evaluate the
arguments to the depth determined by the demand analysis.

In lazy languages evaluation should only occur when necessary. This apparently
sensible rule can be at odds with the goals of performance through parallelism:
if we have parallel processing resources, we wish to use them to do as much work
as possible to shorten execution time \citep{tremblay1995impact}.

Call-by-need semantics forces the compiler to take care in deciding which
sub-expressions can safely be executed in parallel.  Having a simple
parallelisation heuristic such as `compute all arguments to functions in
parallel' can alter the semantics of a non-strict language, introducing
non-termination or runtime errors that would not have occurred during a
sequential execution.


The process of determining which arguments are required for a function is known
as \emph{strictness analysis} \citep{mycroft1980theory}. Since the early 1980's
such analysis has been widely used for reducing the overheads of laziness
\citep{spjDemand}. 

\subsection{Overview}

Because we are working in a lazy language it is not always safe to evaluate the
arguments to a function before we enter the body of a function. This is easy
to see with a simple example; appending two lists\footnote{Here we have used
the na\"{i}ve recursive version, but any correct version of append will
have the same strictness properties.}:

\begin{verbatim}
    append :: [a] -> [a] -> [a]
    append []     ys = ys
    append (x:xs) ys = x : append xs ys
\end{verbatim}

\verb|append| takes two list arguments, a central question that strictness
analysis asks is: How \emph{defined} must the arguments be to \verb|append|
in order for \verb|append| to terminate?

The first hint is that \verb|append| pattern matches on its first argument.
Because the function must be able to distinguish between a \verb|(:)| and a
\verb|[]| we know that the first argument must be defined \emph{at least} to
the outermost constructor. Therefore a use of \verb|append| that is passed
$\bot$ as its first argument will result in non-termination. What about the
second argument, \verb|ys|? Determining how defined \verb|ys| must be turns
out to be impossible without taking into account the \emph{context} that
a call to \verb|append| occurs in. If the first argument is \verb|[]|
then \verb|ys| must be defined to WHNF. However the \verb|(:)| case
guards the recursive call to \verb|append| and, by extension, the use
of \verb|ys|.

The literature on Strictness Analysis is the story of determining these
properties in the general case. We start in \S \ref{sec:idealAnalysis} with
exploring the simplest Strictness Analysis, \emph{Ideal Analysis} on \emph{flat
domains}. We then show how the work was extended to \emph{non-flat domains} in
\S \ref{sec:4pointDomains}. Lastly, we show how the notion of \emph{contexts}
mentioned above are formalised by the use of \emph{projections} from Domain
Theory in section \ref{sec:projections}.


\subsection{Ideal Strictness Analysis}
\label{sec:idealAnalysis}

However, if a function uses the value of an argument within its body it is safe
to evaluate that argument before, or in parallel to, the execution of the body
of the function. In order to determine which arguments can be evaluated in this
way modern compilers use \emph{strictness analysis} \citep{mycroft1980theory}.
More formally, a function $f$ of $n$ arguments

$$
    f\ x_{1} \dots \ x_{i} \ \dots x_{n} = \dots
$$

\noindent is strict in its $i$th argument if and only if

$$
    f\ x_{1} \dots \ \bot \ \dots x_{n} = \bot
$$

What this states is that $f$ is only strict in its $i$th argument if $f$
becomes non-terminating\footnote{In this paper we use the convention that
$\bot$ represents erroneous or non-terminating expressions.} by passing a
non-terminating value as its $i$th argument.

Knowing the strictness information of a function is the first step in automatic
parallelisation. This is because if $f$ is strict in its $i$th argument we do
not risk introducing non-termination (which would not otherwise be present) by
evaluating the $i$th argument in parallel. In other words, evaluating $x_{i}$ in
parallel would only introduce non-termination to the program if evaluating $f$
with $x_{i}$ would have resulted in $f$'s non-termination anyway.

F-Lite has two primitives for taking advantage of strictness information: $par$
and $seq$.
\begin{figure}
\begin{align*}
    &seq \ :: \ a \rightarrow b \rightarrow b &&par \ :: \ a \rightarrow b \rightarrow b \\
    &seq \ x \ y = y                          &&par \ x \ y = y
\end{align*}
\caption{Semantics of \texttt{seq} and \texttt{par}.}
\label{fig:seqandpar}
\end{figure}

Both functions return the value of their second argument. The difference is in
their \emph{side-effects}. $seq$ returns its second argument only \emph{after}
the evaluation of its first argument. $par$ forks the evaluation of its first
argument in a new parallel thread and then returns its second argument; this is
known as \emph{sparking} a parallel task \citep{clack1986four}.

Strictness analysis was a very active research area in the 1980's and the
development of analyses that provide the type of strictness information
outlined above is a well understood problem \citep{mycroft1980theory,
clack1985strictness, burn1986strictness}.  However, as outlined above,
strictness analysis does not provide satisfactory information about complex
data-structures \citep{wadler1987strictness}. This can be remedied by the
use of \emph{projections} to represent \emph{demand}.

\subsection{Abstract Interpretation}

Mycroft introduced the use of abstract interpretation for performing strictness
analysis on call-by-need programs over thirty years ago
\citep{mycroft1980theory}.
Strictness analysis as originally described by Mycroft was only capable of
dealing with a two-point domain (values that are definitely needed, and values
that may or may not be needed). This works well for types that can be
represented by a flat domain (Integer, Char, Bool, etc.)\footnote{Any type that
can be represented as an enumerated type.} but falls short on more complex data
structures. For example, even if we find that a function is strict in a list
argument, we can only evaluate up to the first \verb'cons' safely. For many
functions on lists, evaluating the entire list, or the spine of the list, is
safe; canonical examples are \verb'sum' and \verb'length'.

In order to accommodate this type of reasoning, Wadler developed a
\emph{four-point domain} for the abstract interpretation of list-processing
programs \citep{wadler1987strictness}. However, when extended in the natural way
for general recursive data structures, the size of the domains made finding
fix-points prohibitively costly.

\section{Projections and Contexts}
\label{sec:projections}

So far our discussion of strictness has only involved two levels of
`definedness': a defined value, or $\bot$. This is the whole story when dealing
with \emph{flat} data-structures such as Integers, Booleans or Enumerations.
However, in lazy languages nested data-structures have \emph{degrees} of
definedness.

Take the following example function and value definitions in F-Lite

\begin{centering}
\begin{BVerbatim}
length []     = 0                   sum []     = 0
length (x:xs) = 1 + length xs       sum (x:xs) = x + sum xs

definedList = [1,2,3,4]             infiniteList = [1,2,3...

partialList = [1,2,bot,4]           loop = loop
\end{BVerbatim}
\end{centering}\\

Both \verb-length- and \verb-sum- are functions on lists, but they use lists
differently. \verb-length- does not use the elements of its argument list.
Therefore \verb-length- would accept \verb-definedList- and \verb-partialList-
(which has a non-terminating element) as arguments and still return the correct
value. On the other hand \verb-sum- \emph{needs} the elements of the list,
otherwise it would not be able to compute the sum. For this reason, \verb-sum-
only terminates if it is passed a fully defined list and would result in
non-termination if passed \verb-partialList-. Neither function would terminate
if passed \verb-infiniteList-, since even \verb-length- requires the list to
have a finite length (some functions do not require a finite list, such as
\verb-head-, the function that returns the first element in a list). With
these examples we say that \verb-length- \emph{demands} a finite list, whereas
\verb-sum- \emph{demands} a fully-defined list.

This additional information about a data-structure is extremely useful when
trying to parallelise programs. If we can determine \emph{how much} of a
structure is needed we can then evaluate the structure to that depth in
parallel.

%The first notable attempt at capturing this type of information was
%by Wadler in 1987 \citep{wadler1987strictness}. However, his approach worked
%well on lists but did not scale well to other, more complex data-structures.

The work that introduced this representation of demands was by Wadler and
Hughes \citep{wadler1987projections} using the idea of \emph{projections} from
domain theory.  The technique we use in our compiler is a projection-based
strictness analysis based on the work in Hinze's dissertation
\citep{hinze1995projection}.  Hinze's dissertation is also a good resource for
learning the theory of projection-based strictness analysis.

\section{Demand Analysis}
\label{sec:strictness}
%

In this section we provide a brief overview of the two
predominant techniques for strictness analysis: \emph{abstract interpretation}
and \emph{projection-based} analysis.  We then motivate our decision to use a
projection-based analysis.

\subsection{Projections}

This explosion in cost motivated Wadler and Hughes to propose using
\emph{projections} from domain theory to analyse strictness
\citep{wadler1987projections}.


Projection-based analysis provides two benefits over abstract interpretation:
the ability to analyse functions over arbitrary structures, and a
correspondence with parallel strategies \citep{marlow2010seq, strategies}. This
allows us to use the projections provided by our analysis to produce an
appropriate function to compute the strict arguments in parallel.

Strictness analysis by abstract interpretation asks ``When passing $\bot$ as an
argument is the result of the function call $\bot$?''. Projection-based
strictness analysis instead asks ``If there is a certain degree of demand on
the result of this function, what degree of demand is there on its
arguments?''.

What is meant by `demand'? As an example, the function \verb'length' requires
that the input list be finite, but no more. We can therefore say that
\verb'length' \emph{demands} the spine of the argument list. The function
\verb'append' is a more interesting example:

\begin{alltt}
        append :: [a] -> [a] -> [a]
        append []     ys = ys
        append (x:xs) ys = x : append xs ys
\end{alltt}

By studying the function we can tell that the first argument must be defined to
the first cons, but we cannot know whether the second argument is ever needed. However, what
if the \emph{result} of \verb'append' needs to be a finite list? For example:

\begin{alltt}

    lengthOfBoth :: [a] -> [a] -> Int
    lengthOfBoth xs ys = length (append xs ys)
\end{alltt}

In this case \emph{both} arguments to \verb'append' must be finite. Projections
can be used to formalise this type of context \citep{wadler1987projections,
hinze1995projection}.

\subsection{Semantics of Projections}

Given a domain $D$, a projection on $D$ is a continuous function
$\pi \ : \ D \rightarrow D$ that satisfies

\begin{align}
\pi \sqsubseteq ID \\
\pi \circ \pi = \pi
\end{align}

Equation (3) ensures that a projection can not add any information to a value,
i.e. all projections approximate the identity function. Idempotence (4) ensures
that projecting the same demand twice on a value has no additional effect. This
aligns with our intuition of demand. If we demand that a list is spine-strict,
demanding spine-strictness again does not change the demand on the list.

Because we want the introduction of parallelism to be semantics-preserving we
use the following safety condition for projections:

\begin{equation}
\gamma \ \circ \ f = \gamma \ \circ \ f \ \circ \ \pi
\end{equation}

Given a function $f \ : X \rightarrow Y$, and demand $\gamma$ on the
\emph{result} of $f$, projection-based analysis propagates the demand given by
$\gamma$ to the arguments of $f$. This results in the demand on the
\emph{arguments} of $f$ given by $\pi$.  The analysis aims to find the
\emph{smallest} $\pi$ for each $\gamma$, but approximating towards $ID$ (as
it is always safe to project the identity).

\paragraph{Demands on Primitives}
On unlifted base types, such as unboxed integers, there are two demands,
$ID$ and $BOT$, with the following semantics


\begin{align}
ID \ x \ &= \ x \\
BOT \ x \ &= \ \bot
\end{align}


When an expression is in a $BOT$ context it means that non-termination is
inevitable. You can safely evaluate an expression in this context because there
is no danger of \emph{introducing} non-termination that is not already present.

\paragraph{Demands on Lifted Types} Haskell's non-strict semantics means that
most types we encounter are \emph{lifted} types.  Lifted types represent
possibly unevaluated values. Given a demand $\pi$ on $D$, we can form two
possible demands on $D_{\bot}$, $\pi!$ and $\pi?$; strict lift and lazy lift
respectively. To paraphrase Kubiak et al.: $\pi!$ means we will definitely need
the value demanded by this projection, and we will need $\pi$'s worth of it
\citep{kubiak}. $\pi?$ does not tell us whether we need the value or not, but if
we \emph{do} need the value, we will need it to satisfy $\pi$'s demand.

\paragraph{Demands on Products} A projection representing a demand on a product
can be formed by using the $\otimes$ operator with the following semantics

\begin{align*}
\langle \pi_{1} \otimes \dots \otimes \pi_{n} \rangle \ \bot &= \bot \\
\langle \pi_{1} \otimes \dots \otimes \pi_{n} \rangle \ 
\langle x_{1}, \dots, x_{n} \rangle &= \langle \pi_{1} x_{1}, \dots, \pi_{n} x_{n} \rangle
\end{align*}

\paragraph{Demands on Sums} If projections are functions on a domain, then
$\oplus$, the operator that forms projections on sum-types performs the case-analysis.

\begin{align*}
[ID_{True} \oplus ID_{False}]  \ True &= True \\
[ID_{True} \oplus BOT_{False}] \ False &= \bot
\end{align*}

\begin{figure}
\begin{align*}
    d ::=&\ BOT              & \text{Bottom (hyperstrict)} \\
        |&\ ID               & \text{Top (the identity)} \\
        |&\ \langle d_{1} \otimes d_{2} \dots \otimes d_{n} \rangle   & \text{Products} \\ 
        |&\ [d_{1} \oplus d_{2} \dots \oplus d_{n}]    & \text{Sums} \\ 
        |&\ \mu\beta . d     & \text{Recursive Demands} \\
        |&\ d?               & \text{Strict Lift} \\
        |&\ d!               & \text{Lazy Lift}
\end{align*}
\caption{Abstract Syntax for Contexts of Demand}
\label{fig:ContextAST}
\end{figure}


Figure \ref{fig:ContextAST} presents a suitable abstract syntax for projections
representing demand.  This form was introduced by Kubiak et al. and used in
Hinze's work on projection-based analyses \citep{kubiak, hinze1995projection}.
We have omitted the details on the representation of context variables (for
polymorphic demands), for a comprehensive discussion we suggest Chapter 6 of Hinze's
dissertation \citep{hinze1995projection}.

In short, projections representing demand give us information about how defined
a value must be to satisfy a function's demand on that value. Knowing that a
value is definitely needed, and to what degree, allows us to evaluate the value
before entering the function.

\subsection*{Example Projections}

Because our primitives can be modelled by a flat domain (just $ID$ and $BOT$),
our lattice of projections corresponds with the two-point domain used in
abstract interpretation.

\hfill$\Box$

For pairs of primitive values, possible contexts include:
\begin{align}
[\langle ID? \otimes ID? \rangle] \label{IDPairs} \\
[\langle ID! \otimes ID? \rangle] \label{FSTPairs}
\end{align}


As Haskell's types are sums of products, pairs are treated as sums with only
one constructor.  For product types each member of the product is lifted.
Context \ref{IDPairs} is the top of the lattice for pairs, accepting all
possible pairs. Context \ref{FSTPairs} requires that the first member be
defined but does not require the second element. This is the demand that
\verb-fst- places on its argument.

\hfill$\Box$

For polymorphic lists there are 7 principal contexts; 3 commonly occurring contexts are:

\begin{align}
    \mu\beta.&[ID \oplus \langle \gamma? \otimes \beta?\rangle] \label{IDList} \\
    \mu\beta.&[ID \oplus \langle \gamma? \otimes \beta!\rangle] \label{FINList} \\
    \mu\beta.&[ID \oplus \langle \gamma! \otimes \beta!\rangle] \label{FULLList}
\end{align}


Here $\mu$ binds the name for the `recursive call' of the projection and
$\gamma$ is used to represent an appropriate demand for the element type of the
list.  An important point is that this representation for recursive contexts
restricts the representable contexts to \emph{uniform projections}: projections
that define the same degree of evaluation on each of their recursive components
as they do on the structure as a whole. The detailed reason for this
restriction is given on page 89 of Hinze \citep{hinze1995projection}. This
limitation does not hinder the analysis significantly as many functions on
recursive structures are themselves uniform.

With this in mind Context \ref{IDList} represents a lazy demand on the list,
Context \ref{FINList} represents a \emph{tail strict} demand, and Context
\ref{FULLList} represents a \emph{head and tail} strict demand on the list.

\hfill$\Box$

It will be useful to have abbreviation for a few of the contexts on lists. These
abbreviation are presented in Figure \ref{contexts}.

\begin{figure}[h!]
\begin{itemize}
    \item[] ID: accepts all lists
    \item[] T (tail strict): accepts all finite lists
    \item[] H (head strict): accepts lists where the head is defined
    \item[] HT: accepts finite lists where every member is defined
\end{itemize}
\caption{Four contexts on lists as described in \citep{wadler1987projections}.}
\label{contexts}
\end{figure}

We can now say more about the strictness properties of \verb'append'. The
strictness properties of a function are presented as a \emph{context
transformer} \citep{hinze1995projection}. 

\begin{align*}
    &append(ID) &\rightarrow &&ID!&;ID? \\
    &append(T)  &\rightarrow &&T!&;T! \\
    &append(H)  &\rightarrow &&H!&;H? \\
    &append(HT) &\rightarrow &&HT!&;HT!
\end{align*}

This can be read as ``If the demand on the result of \verb-append- is $ID$
then the first argument is strict with the demand $ID$ and the second
argument is lazy, but if it \emph{is} needed, it is with demand $ID$.

\hfill$\Box$

Following Hinze \citep{hinze1995projection} we construct projections
for every user-defined type. Each projection represents a
specific strategy for evaluating the structure, as we shall define in section
\ref{sec:proAndStrat}.  This provides us with the ability to generate
appropriate parallel strategies for arbitrary types. Using a
projection-based strictness analysis, we avoid the exponential blowup
of domains required for abstract interpretation.

\subsection*{Strategies}

With the more sophisticated information provided by projection-based analysis,
we require more than simply $par$ and $seq$. To this end we use the popular
technique of \emph{strategies} for parallel evaluation \citep{strategies,
marlow2010seq}. Strategies are designed to evaluate structures up to a certain
depth in parallel to the use of those structures. Normally, strategies are
written by the programmer for use in hand-parallelised code. In order to
facilitate auto-parallelisation we have developed a method to \emph{derive} an
appropriate strategy from the information provided to us by projection-based
strictness analysis. The rules for the derivation are presented as a
denotational semantics and can be found in our earlier work \citep{calderon}.

\section{The Granularity Problem}

We have now discussed how we find the parallelism that is implicit in our
program, but none of the analysis we provide determines whether the safe
parallelism is \emph{worthwhile}. Often static analysis will determine that a
certain structure is \emph{safe} to compute in parallel, but it is very
difficult to know when it is actually of any benefit. Parallelism has overheads
that require the parallel tasks to be substantial enough to make up for the
cost. A \emph{fine-grained} task is unlikely to require more computation than
the cost of sparking and managing the thread, let alone the potential to
interrupt productive threads \citep{hammond2000research, hogen1992automatic}.

One of the central arguments in our work is that static analysis \emph{alone}
is insufficient at finding both the implicit parallelism and determining whether
the introduced parallelism is substantial enough to warrant the overheads.

Our proposal is that the compiler should \emph{run} the program and use the
information gained from running it (even if it only looks at overall execution
time) to \emph{remove} the parallelism that is too fine-grained. By doing this
we shift the burden of the granularity problem away from our static analysis
and onto our search techniques. This way our static analysis is only used to
determine the safe parallel expressions, and not the granularity of the
expressions.
