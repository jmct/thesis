In this section we will motivate and discuss the benefits and drawbacks of
implicit parallelism in a lazy purely functional language. We will also give
a high-level overview of \emph{strictness analysis} which allows us to find safe
parallelism in lazy languages.

\subsection{Background}

Research into parallelism in lazy purely functional languages has a long
history that dates back to the early work on lazy functional languages
\citep{hughes:thesis, vGMachine, dutchBook, SPJ:PIFPL}\footnote{For a
comprehensive review we suggest \citep{hammond2000research}}. Non-strictness
makes it difficult to reason about when expressions are evaluated. This forces
the programmer to avoid the use of arbitrary side-effects. The resulting purity
means that functions in pure functional languages are \emph{referentially
transparent}, or the result of a function depends only on the values of its
arguments (i.e.  there is no global state that could effect the result of the
function or be manipulated by the function).

Purity alone is of huge benefit when dealing with parallelism. Because
functions do not rely on anything but their arguments the only communication
between threads necessary is the result of the thread's computation, which is
shared via the program's graph using the same mechanism used to implement
laziness \citep{SPJ:PIFPL}.

Laziness, while forcing the programmer to be pure (which is a boon to
parallelism), is an inherently sequential evaluation strategy. Lazy evaluation
only evaluates expressions when they are \emph{needed}. This is what allows for
the use of infinite data structures, only what is needed will be computed.

The two reductions of $sqr$ in Figure \ref{fig:eagerandlazy} illustrate the key
differences between lazy evaluation and eager, or strict, evaluation


\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{align*}
     \noalign{$\text{\underline{Eager Evaluation}}$}
     &sqr\ (5*5) \\
  =\ &sqr\ 25 \\
  =\ &let\ x\ =\ 25\ in\ x * x \\
  =\ &25 * 25 \\
  =\ &625
\end{align*}
\begin{align*}
     \noalign{$\text{\underline{Lazy Evaluation}}$}
     &sqr\ (5*5) \\
  =\ &let\ x\ =\ 5*5\ in\ x * x \\
  =\ &let\ x\ =\ 25\ in\ x * x \\
  =\ &25 * 25 \\
  =\ &625
\end{align*}
\end{multicols}
\caption{Eager and Lazy evaluation order for squaring a value.}
\label{fig:eagerandlazy}
\end{figure}

In the case of eager evaluation the argument to $sqr$ is evaluated
\emph{before} entering the function body. For lazy evaluation the argument is
passed as a suspended computation that is only \emph{forced} when the value is
needed (in this case when $x$ is needed in order to multiply $x*x$). Notice
that under lazy evaluation $5*5$ is only evaluated once, even though it is
used twice in the function. This is due to the \emph{sharing} of the result.
This is why laziness is often described as call-by-need \emph{with sharing}
\citep{hammond2000research}.

\medskip

In the case of $sqr$ in Figure \ref{fig:eagerandlazy}, both eager and lazy
evaluation required the same number of \emph{reductions} to compute the final
result. This is not always the case; take the following function definitions
\begin{align*}
    &bot \ :: \ Int\ \rightarrow\ Int \\
    &bot\ x\ =\ x + bot \\
    \quad & \\
    &const\ :: \ a\ \rightarrow\ b\ \rightarrow\ a \\
    &const\ x\ y\ =\ x
\end{align*}
\label{fig:botAndConst}

In an eager language the expression $const\ 5\ bot$ will never terminate,
while it would return $5$ in a lazy language as only the first argument
to $const$ is actually \emph{needed} in its body.

\medskip


This tension between the call-by-need convention of laziness with parallelism's
desire to evaluate expressions \emph{before} they are needed is well known
\citep{tremblay1995impact}. The most succesful method of combating this tension
is through the use of \emph{strictness analysis} \citep{mycroft1980theory,
wadler1987projections, hinze1995projection}.


\subsection{Strictness, Demand Context, and Strategies}

Here we will describe the method by which we identify the \emph{safe}
parallelism in F-Lite programs and arrange for the evaluation of these
expressions in parallel. The \emph{strictness} properties of a function
determine which arguments are definitely needed for the function to terminate,
whereas the \emph{demand} on an argument tells us \emph{how much} of the
argument's structure is needed. \emph{Strategies} are functions that evaluate
their argument's structure to a specific depth. By analysing the program for
strictness and demand information, we can then generate strategies for the
strict arguments to a function and evaluate the strategies in parallel to the
body of the function. The strategies we generate will only evaluate the
arguments to the depth determined by the demand analysis.

\subsubsection*{Strictness}

Because we are working in a lazy language it is not always safe to evaluate the
arguments to a function before we enter the body of a function. However, if a
function uses the value of an argument within its body it is safe to evaluate
that argument before, or in parallel to, the execution of the body of the
function. In order to determine which arguments can be evaluated in this way
modern compilers use \emph{strictness analysis} \citep{mycroft1980theory}. More
formally, a function $f$ of $n$ arguments

$$
    f\ x_{1} \dots \ x_{i} \ \dots x_{n} = \dots
$$

\noindent is strict in its $i$th argument if and only if

$$
    f\ x_{1} \dots \ \bot \ \dots x_{n} = \bot
$$

What this states is that $f$ is only strict in its $i$th argument if $f$
becomes non-terminating\footnote{In this paper we use the convention that
$\bot$ represents erroneous or non-terminating expressions.} by passing a
non-terminating value as its $i$th argument.

Knowing the strictness information of a function is the first step in automatic
parallelisation. This is because if $f$ is strict in its $i$th argument we do
not risk introducing non-termination (which would not otherwise be present) by
evaluating the $i$th argument in parallel. In other words, evaluating $x_{i}$ in
parallel would only introduce non-termination to the program if evaluating $f$
with $x_{i}$ would have resulted in $f$'s non-termination anyway.

F-Lite has two primitives for taking advantage of strictness information: $par$
and $seq$.
\begin{figure}
\begin{align*}
    &seq \ :: \ a \rightarrow b \rightarrow b &&par \ :: \ a \rightarrow b \rightarrow b \\
    &seq \ x \ y = y                          &&par \ x \ y = y
\end{align*}
\caption{Semantics of \texttt{seq} and \texttt{par}.}
\label{fig:seqandpar}
\end{figure}

Both functions return the value of their second argument. The difference is in
their \emph{side-effects}. $seq$ returns its second argument only \emph{after}
the evaluation of its first argument. $par$ forks the evaluation of its first
argument in a new parallel thread and then returns its second argument; this is
known as \emph{sparking} a parallel task \citep{clack1986four}.

Strictness analysis was a very active research area in the 1980's and the
development of analyses that provide the type of strictness information
outlined above is a well understood problem \citep{mycroft1980theory,
clack1985strictness, burn1986strictness}.  However, as outlined above,
strictness analysis does not provide satisfactory information about complex
data-structures \citep{wadler1987strictness}. This can be remedied by the
use of \emph{projections} to represent \emph{demand}.

\subsubsection*{Demand}

So far our discussion of strictness has only involved two levels of
`definedness': a defined value, or $\bot$. This is the whole story when dealing
with \emph{flat} data-structures such as Integers, Booleans or Enumerations.
However, in lazy languages nested data-structures have \emph{degrees} of
definedness.

Take the following example function and value definitions in F-Lite

\begin{centering}
\begin{BVerbatim}
length []     = 0                   sum []     = 0
length (x:xs) = 1 + length xs       sum (x:xs) = x + sum xs

definedList = [1,2,3,4]             infiniteList = [1,2,3...

partialList = [1,2,bot,4]           loop = loop
\end{BVerbatim}
\end{centering}\\

Both \verb-length- and \verb-sum- are functions on lists, but they use lists
differently. \verb-length- does not use the elements of its argument list.
Therefore \verb-length- would accept \verb-definedList- and \verb-partialList-
(which has a non-terminating element) as arguments and still return the correct
value. On the other hand \verb-sum- \emph{needs} the elements of the list,
otherwise it would not be able to compute the sum. For this reason, \verb-sum-
only terminates if it is passed a fully defined list and would result in
non-termination if passed \verb-partialList-. Neither function would terminate
if passed \verb-infiniteList-, since even \verb-length- requires the list to
have a finite length (some functions do not require a finite list, such as
\verb-head-, the function that returns the first element in a list). With
these examples we say that \verb-length- \emph{demands} a finite list, whereas
\verb-sum- \emph{demands} a fully-defined list.

This additional information about a data-structure is extremely useful when
trying to parallelise programs. If we can determine \emph{how much} of a
structure is needed we can then evaluate the structure to that depth in
parallel.

%The first notable attempt at capturing this type of information was
%by Wadler in 1987 \citep{wadler1987strictness}. However, his approach worked
%well on lists but did not scale well to other, more complex data-structures.

The work that introduced this representation of demands was by Wadler and
Hughes \citep{wadler1987projections} using the idea of \emph{projections} from
domain theory.  The technique we use in our compiler is a projection-based
strictness analysis based on the work in Hinze's dissertation
\citep{hinze1995projection}.  Hinze's dissertation is also a good resource for
learning the theory of projection-based strictness analysis.

\subsubsection*{Strategies}

With the more sophisticated information provided by projection-based analysis,
we require more than simply $par$ and $seq$. To this end we use the popular
technique of \emph{strategies} for parallel evaluation \citep{strategies,
marlow2010seq}. Strategies are designed to evaluate structures up to a certain
depth in parallel to the use of those structures. Normally, strategies are
written by the programmer for use in hand-parallelised code. In order to
facilitate auto-parallelisation we have developed a method to \emph{derive} an
appropriate strategy from the information provided to us by projection-based
strictness analysis. The rules for the derivation are presented as a
denotational semantics and can be found in our earlier work \citep{calderon}.

\subsection{The Granularity Problem}

We have now discussed how we find the parallelism that is implicit in our
program, but none of the analysis we provide determines whether the safe
parallelism is \emph{worthwhile}. Often static analysis will determine that a
certain structure is \emph{safe} to compute in parallel, but it is very
difficult to know when it is actually of any benefit. Parallelism has overheads
that require the parallel tasks to be substantial enough to make up for the
cost. A \emph{fine-grained} task is unlikely to require more computation than
the cost of sparking and managing the thread, let alone the potential to
interrupt productive threads \citep{hammond2000research, hogen1992automatic}.

One of the central arguments in our work is that static analysis \emph{alone}
is insufficient at finding both the implicit parallelism and determining whether
the introduced parallelism is substantial enough to warrant the overheads.

Our proposal is that the compiler should \emph{run} the program and use the
information gained from running it (even if it only looks at overall execution
time) to \emph{remove} the parallelism that is too fine-grained. By doing this
we shift the burden of the granularity problem away from our static analysis
and onto our search techniques. This way our static analysis is only used to
determine the safe parallel expressions, and not the granularity of the
expressions.
