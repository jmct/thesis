We propose a new technique for exploiting the inherent parallelism in lazy
functional programs. Known as \emph{implicit parallelism}, the goal of writing
a sequential program and having the compiler improve its performance by
determining what can be executed in parallel has been studied for many years.
Our technique abandons the idea that a compiler should accomplish this feat in
`one shot' with static analysis and instead allow the compiler to
\emph{improve} upon the static analysis using iterative feedback.

We demonstrate that iterative feedback can be relatively simple when the source
language is a lazy purely functional programming language. We present three
main contributions to the field: the automatic derivation of parallel
strategies from a demand on a structure, and two new methods of
feedback-directed auto-parallelisation. The first method treats the runtime of
the program as a \emph{black box} and uses the `wall-clock' time as a fitness
function to guide a heuristic search on bitstrings representing the parallel
setting of the program. The second feedback approach is \emph{profile
directed}. This allows the compiler to use profile data that is gathered by the
runtime system as the program executes. This allows the compiler to determine
which threads are not worth the overhead of creating them.

Our results show that the use of feedback-directed compilation can be a good
source of refinement for the static analysis techniques that struggle to
account for the cost of a computation. This lifts the burden of `is this
parallelism worthwhile?' away from the static phase of compilation and to the
runtime, which is better equipped to answer the question.
