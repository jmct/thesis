After parsing the next stage of the compiler applies a
defunctionalising\footnote{Some have taken issue with our use of the term
`defunctionalisation'. Many see defunctionalisation as the \emph{specific}
transformation introduced by Reynolds \citep{reynoldsDefun} to remove
higher-order functions from programs. However, we feel that defunctionalisation
is the \emph{concept} of transforming a higher-order program to a first-order
program. Reynold's transformation is but one instantiation of this concept.}
transformation to the input programs. Our defunctionalisation method is limited
in scope, but sufficient for our purposes. It specialises higher-order
functions defining separate instances for different functional arguments. We
are careful to preserve sharing during this transformation. Here we give our
motivation for introducing this transformation.

A significant motivator is that our chosen strictness analysis can not cope
with higher-order programs. However, it would certainly be possible to extend
such an analysis to higher-order function if required, but our use-case
provides other incentives to remove higher-order functions. When taken together
we do not see defunctionalisation as a compromise but as an enabling mechanism
for implicit parallelism.

Central to our thesis is the concept of \verb-par- placement within a program.
Each \verb-par- application can be identified by its \emph{position} in the
AST. In a higher-order program basing our parallelism on the location of a
\verb-par- would very likely lead to undesirable consequences. This is because
parallelising the application of a function to its arguments becomes more
difficult when the function in question is unknown. Take \<foo\> below, which
takes a functional argument \<g\>.

\begin{haskell*}
\hscom{`g' is a functional argument} \\
foo g = \dots g e_{1} e_{2} \dots
\end{haskell*}

Our goal is to parallelise the evaluation of \<e_{1}\> and \<e_{2}\> when it is
safe to do so, but because we lack concrete information about the strictness of
\<g\> it is not possible to know when it is.

By defunctionalising we gain an new instance of \<foo\> for every unique
functional argument. If \<foo\> was passed the functions \<bar\> and \<qux\>
in our program that would leave us with the following function defintions.

\begin{haskell*}
foo_bar &=& \dots bar e_{1} e_{2} \dots \\
foo_qux &=& \dots qux e_{1} e_{2} \dots
\end{haskell*}

Now we are able to analyse the instantiations of \<foo\> independently. If
\<bar\> is strict in both its arguments but \<qux\> is not we face no dilema.

\begin{haskell*}
foo_bar &=& \dots \hslet{e^{'}_{1} &= e_{1}\\
                         e^{'}_{2} &= e_{2}}{%
                         e^{'}_{1} `par` e^{'}_{2} `par` bar e_{1} e_{2}
                         } \dots \\
\quad&\quad&\quad \\
foo_qux &=& \dots qux e_{1} e_{2} \dots
\end{haskell*}

We can have out cake and eat it too! We retain our safety but are able to
maximise the parallelism that our compiler introduces.

In addition to allowing the compiler to introduce more parallelism,
defunctionalisation aides in the iterative portion of our work.  For example,
a common pattern in parallel programs is to introduce a parallel version of the
\verb-map- function

\begin{alltt}
    parMap :: (a -> b) -> [a] -> [b]
    parMap f []     = []
    parMap f (x:xs) = let y = f x
                      in y `par` y : parMap f xs
\end{alltt}

There is inevitably some overhead associated with evaluation of a \verb-par-
application, and of sparking off a fresh parallel thread.  So if the
computation \verb-f x- is inexpensive, the parallelism may not provide any
benefit and could even be detrimental. As \verb-parMap- may be used throughout
a program it is possible that there are both useful and detrimental parallel
applications for various functional arguments: \verb-parMap f- may provide
useful parallelism while \verb-parMap g- may cost more in overhead than we gain
from any parallelism.  Unfortunately when this occurs we are unable to switch
off the \verb-par- for \verb-parMap g- without losing the useful parallelism of
\verb-parMap f-. This is because the \verb-par- annotation is within the body
of \verb-parMap-. By specialising \verb-parMap- we create two separate
functions: \verb-parMap_f- and \verb-parMap_g-, with distinct \verb-par-
annotations in \emph{each} of the instances of \verb-parMap-.

\begin{alltt}
    parMap_f []     = []
    parMap_f (x:xs) = let y = f x
                      in y `par` y : parMap_f xs

    parMap_g []     = []
    parMap_g (x:xs) = let y = g x
                      in y `par` y : parMap_g xs
\end{alltt}


After defunctionalisation we can determine the usefulness of parallelism
in each case independently. The plan is to deactivate the \verb-par- for the
inexpensive computation, \verb-g x-, without affecting the parallel application
of the worthwhile computation, \verb-f x-.

\subsection{How We Defunctionalise}

Our defunctionaliser makes the following set of assumptions:

\begin{itemize}
  \item Algebraic data structures are first-order (no functional components)
  \item The patterns on the left-hand side of a declaration have been compiled
        into case expressions
  \item Functions may have functional arguments but their definitions must be
        arity-saturated and return data-value results
  \item No explicit lambdas in the program, but partial applications are permitted
\end{itemize}

With these assumptions in mind, the rules for defunctionalisation are presented
in Figure \ref{defunRules}. These rules are applied to the AST in a
\emph{bottom up} fashion. This allows the transformation to assume that the
arguments to partially applied functions (like $e'_{1}$ in (1)) have already
been defunctionalised.

\paragraph{Example}

Take \verb-reverse- defined as an instance of \verb-foldl-:

\begin{verbatim}
    reverse xs  =  foldl (flip Cons) Nil xs
\end{verbatim}

this becomes

\begin{verbatim}
    reverse xs  =  foldl_flip_Cons Nil xs

    foldl_flip_Cons z xs 
        = case xs of
            Nil       -> z
            Cons y ys ->
                foldl_flip_Cons (flip_Cons z y) ys

    flip_Cons xs x = Cons x xs
\end{verbatim}

\hfill$\Box$

\begin{figure}[t]
 \begin{align}
  \begin{split}
   &f \ e_{1} \dots e_{i-1}\ (g \ e'_{1} \dots e'_{m})\ e_{i+1} \dots e_{\#f} \qquad 0 \leq m < \#g\\
   &\quad \implies f_{\langle i,g,m\rangle} \ e_{1} \dots e_{i-1}\ e'_{1} \dots e'_{m} \ e_{i+1} \dots e_{\#f}
  \end{split}\\[10pt]
  \begin{split}
  &f \ x_{1} \dots \ x_{n} \ = e \\
  &\quad \implies f_{\langle i,g,m\rangle} \ x_{1} \dots x_{i-1}\ y_{1} \dots y_{m}\ x_{i+1} \dots x_{n} \\
  & \qquad \qquad \quad = e[x_{i}/g\ y_{1} \dots y_{m}]
  \end{split}
 \end{align}
\caption[Defunctionalisation Rules]{%
        Rules for Defunctionalisation. $\#f$ and $\#g$ represent the arities of the functions.
        (1) refers to the transformation at the \emph{call site},
        (2) describes the transformation of the definition, creating a new version of $f$ that has
        been specialised at its $i$th argument with function $g$ and $m$ arguments to $g$.}
\label{defunRules}
\end{figure}

The defunctionalisation rules are admittedly simple and we do not claim a
contribution with this method. A production system would likely need to develop
a more robust method of defunctionalisation or opt for a higher-order
strictness analysis.

Now that our programs are defunctionalised we are able to take advantage of our
chosen strictness analysis.
