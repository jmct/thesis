\documentclass[justified]{tufte-book}

\title{Implicit Parallelism: Trying Again}

\author{Jos\'{e} Manuel Calder\'{o}n Trilla}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{breakcites}
%\usepackage{subcaption}
\usepackage{alltt}
\usepackage{color}

\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{url}
\usepackage{siunitx}
\usepackage{etoolbox}

% Tufte-book imports natbib, so we can use \citet and \citep without
% importing the package
%\usepackage[round]{natbib}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

%This is the stuff for semantic equations%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\robustify\bfseries
\newsavebox{\sembox}
\newlength{\semwidth}
\newlength{\boxwidth}

\newcommand{\Sem}[1]{%
\sbox{\sembox}{\ensuremath{#1}}%
\settowidth{\semwidth}{\usebox{\sembox}}%
\sbox{\sembox}{\ensuremath{\left[\usebox{\sembox}\right]}}%
\settowidth{\boxwidth}{\usebox{\sembox}}%
\addtolength{\boxwidth}{-\semwidth}%
\left[\hspace{-0.3\boxwidth}%
\usebox{\sembox}%
\hspace{-0.3\boxwidth}\right]%
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\defineword}[2]{%
\begin{description}%
    \item{\textbf{#1}} \hfill \\%
        {#2}%
\end{description}%
}

\newcommand{\sigval}[1]{\bfseries #1}
\begin{document}

\frontmatter

%\blankpage

\maketitle

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}

    There is a very common refrain that programmers use. It goes something like
    this: ``If you want a $[X]$ program, you must $[Y]$''. We can choose
    appropriate $X$'s and $Y$'s to prove a point about the difficulty of
    programming. Here are some common examples:

    \begin{itemize}
      \item ``If you want a fast program, you must write it in C''
      \item ``If you want an efficient program, you must write it in
                assembler''
      \item ``If you want a performant program, you must use
                cache-conscious structures''
      \item ``If you want a parallel program, you must write a parallel
                program''
    \end{itemize}

    This thesis is concerned with the last of these examples. What does it mean?
    For many, the idea that a compiler can \emph{automatically} parallelise a
    program that was written as a sequential one is a pipe-dream. This version
    of the refrain attempts to emphasise the point that utilising parallelism
    require \emph{active thought and action} by the programmer, we can not
    get parallelism `for free'.
    
    We seek to show that this is not always that case. We do this by attempting
    the inverse of the refrain: writing a compiler that is able to take a
    \emph{sequential} program and transform it into a better
    performing\footnote{This is key!} \emph{parallel} program.  A system that
    can achieve this goal is said to take advantage of a program's
    \emph{implicit}, or \emph{inherent}, parallelism.

    \defineword{Implicit Parallelism}{The potential parallelism that is present
        in a program without the need for any annotations, calling of parallel
        functions, or use of parallel libraries.}
    
    \section{Lazy Languages and Parallelism}
    \input{Intro/Introduction.tex}



    \section{Thesis Roadmap}

\chapter{Parallelism in a Functional Language}

    This chapter explores previous approaches to parallelism in a
    functional language, both implicit and explicit. We do this so that
    we may better understand the trade-offs we accept when utilising
    implicit parallelism and the difficulties involved.


    General overview of why functional programming is considered `good' for
    parallelism \citep{hughes:thesis}.
    \section{Functional Programming and Parallelism}
    \input{Background/parFuncProg.tex}
    
    %Review of Graph Reduction. Needs to focus more on the G-Machine and our
    %implementation. Will we convert to spineless G-Machine?
    \section{Graph Reduction}
    \input{Background/funcProgGraphRedctn.tex}

\chapter{The Discovery and Placement of Safe Parallelism}

    Before we can run our automatically parallelised programs, we must develop
    methods and techniques for the compiler to \emph{find} and \emph{express}
    the parallelism that is implicit in our programs. This is the main focus
    of this chapter.

    \section{Defunctionalisation (or Higher-Order Specialisation)}
    \input{Static-Analysis/Defunctionalisation.tex}

    \section{Strictness Analysis}
    \input{Static-Analysis/StrictnessAnalysis.tex}

    \section{Expressing Need, Strategically}
    \input{Static-Analysis/StrategyDerivation.tex}
    
\chapter{Bitstring Search}

    This chapter will talk about the work on blind-search with Simon.

    \section{Introduction}
    \input{Blind/Intro.tex}

    \section{Overview}
    \label{sec:blind-Overview}
    \input{Blind/Overview.tex}

    \section{Implicit Parallelism in Functional Languages}
    \label{sec:blind-ParFunc}
    \input{Blind/ParFunc.tex}

    \section{Experimental Setup and Results}
    \label{sec:blind-Results}
    \input{Blind/Results.tex}

    \section{Conclusions of Bitstring Searching}
    \label{sec:blind-Conclusion}
    \input{Blind/Conclusions.tex}

\chapter{Informed Search}

    New things here.
    \label{sec:informed-search}
    \input{Informed/iip.tex}

\chapter{Conclusions}

    And.... we're done.

\backmatter

\bibliography{literature}
\bibliographystyle{plainnat}

\end{document}
