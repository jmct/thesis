    1978 Turing Award winner, John Backus, used his acceptance speech to ask the
question: ``Can Programming be liberated from the von Neumann Style?''
\citep{HistoryOfHaskell}. The crux of his argument was that the traditional
and ubiquitous architecture of the day was not suitable for the eventual shift
to parallelism and the performance gains that could be achieved through
parallelism's use. This fed the
interest in novel computer architectures that would more readily support
parallelism.

More declarative languages, and particularly functional languages, were seen as
being better suited to Backus' challenge than more traditional imperative
languages. This is because many imperative languages were designed for the
simplicity of compilation. They free the programmer from repetitive
bookkeeping, such as saving registers for function calls and saving the
intermediate computations in an expression, but do not conceal all aspects of
the underlying machine. Most crucially, they allow arbitrary mutation and
side-effects. This limits the amount of parallelism possible because the result
of a function call can depend on more than the values of the passed arguments.

    Work had already been carried out on non-von Neumann architectures
before that time, however, much of it was in the form of abstract machines
that functioned `on top' of the von Neumann architecture \citep{turnerHistory}.

In the early 70's the idea of graph reduction is introduced \citep{wadsworth}
and with it, the concept of lazy evaluation was ready to be formalised.
%\citep{lazyCons, HistoryOfHaskell}
Lazy evaluation has its roots in papers by Henderson and Morris, and Friedman
and Wise \citep{turnerHistory}.  People began to think of ways to better
implement graph reduction machines.  A big breakthrough for software reduction
was Turner's SKI-Combinator reduction scheme in 1979 \citep{turnerHistory,
clackbook}.

    In the 1980's we saw a great interest in parallel architectures for
functional languages. The two main conferences in the field were `LISP and
Functional Programming' (which was held on even years) and `Functional
Programming and Computer Architecture' (which was held on odd years).

    Several novel architectures were developed with the hopes that they could
surpass stock hardware in their performance. This line of research continued
through the 80's and into the early 90's \citep{Alice, GRIP, clackbook,
PFPAnIntro}.

The G-Machine in '84 showed that lazy functional languages (which had always
been considered inefficient and bound to being interpreted) could be
implemented in an efficient manner on stock hardware via the compilation of
supercombinators to machine code. However, the abstract machine could also be
used in the implementation of a novel architecture itself
\citep{Augustsson:LazyMLCompiler}.

    The 1990's saw a decline in the amount of research aimed at parallel
functional programming. This was mainly due to the results from earlier research
being less successful than had been hoped (some novel architectures did see good
results, but they could not keep up with the improvements seen in the sequential
hardware sphere) \citep{PFPAnIntro, clackbook}.

The late 90's and the 2000's saw a resurgence in the interest in parallel
functional programming. While still constrained by the von Neumann bottleneck,
interest in this architecture is high because of the ubiquity of multi-core
computers today. Generally, many of the techniques discussed earlier are used,
(GHC using the STG-Machine, for example) \citep{buckwheat, haskellSharedMem}.


While still suffering from the von Neumann bottleneck, the utilisation of
multiple cores in lazy functional languages is widely considered to be a
success. Many high-level abstractions have been introduced that provide the
programmer with powerful tools to exploit their multi-core systems. Strategies
\citep{strategies}, parallel skeletons \citep{skeletons}, the Par-Monad
\citep{marlow2011monad}, and Repa \citep{repa} are among the most prominent
successes. We will explore these in more depth in Section \ref{sec:Approaches}.
