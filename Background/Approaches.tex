When looking at parallel programming, it is important to make the distinction
between concurrency and parallelism.  Concurrency embodies the idea of multiple
workers (threads, computers, agents, etc.) making progress on independent
tasks. A standard example for concurrency is a modern web-server. Each
connection to the web-server can be thought of as an independent sub-task of
the program. The web-server does not have to be run on a multi-core machine for
the concurrency to take place. A single-core machine is capable of running
concurrent threads through scheduling and context switching.

Parallelism describes the simultaneous execution of tasks with the purpose of
achieving a gain in performance. Tasks that can be easily divided into
independent sub-tasks can easily be made into parallel algorithms. For example,
if one wanted to compute the monthly average temperature for a given year, each
month could be computed independently.  If enough of these independent
computations happen simultaneously there can be a substantial improvement in
the program's wall-clock speed. Ideally, the increase in performance would
scale at the same rate as the available parallel machinery (2$x$ performance
with two processors, 5$x$ performance with 5 processors). Unfortunately, there
are some unavoidable factors that prevent this ideal from being realised
\citep{hughes:thesis,HistoryOfHaskell,PFPAnIntro}. The most basic fact
preventing this ideal is that the executing machinery (whether virtual or
physical) will necessarily introduce some overhead in the generation and
management of parallel threads of execution \citep{PeytonJones:IFL}. Beyond
that, it is unusual for a program to be perfectly parallel except in trivial
cases. Most parallel programs exhibit non-uniform parallelism and complex data
dependencies. This results in programs where parallel threads vary greatly in
their processing time and contain threads of execution that will depend on the
results of other threads. This results in threads having to wait for the result
of another thread before commencing (known as blocking).

\subsection{Haskell}

Haskell is a lazy functional language benefiting from constant development
since its inception in 1987\footnote{1987 was when the academic community
decided that an `standard' language was needed to unify the study of lazy
functional programming\citep{HistoryOfHaskell}, however, the first Haskell
report was not published until 1990 \citep{Haskell98Book}}
\citep{HistoryOfHaskell, Haskell98Book}. Haskell as defined by the Haskell
Report \citep{Haskell98Book} does not feature parallelism as part of the language.
However, functional programming has a long history of parallel implementations
and Haskell is no different in this regard. Even the early implementations of
Haskell had facilities for parallel computation.

\paragraph{Haskell Parallelism}

The Glasgow Haskell Compiler (GHC) has extensive parallelism and concurrency
support; this is part of what makes the compiler a popular implementation
\citep{HistoryOfHaskell}. While our eventual goal is to have access to
compilers that take advantage of the implicit parallelism in our programs, it
is useful to understand the tools and libraries that enable programmers to use
\emph{explicit} parallelism in their Haskell programs. Some of these techniques
are well-established and have decades of use and experience to draw from.

\subsection{Explicit Parallelism with \texttt{par} and \texttt{seq}}
\label{sec:parAndSeq}

The most popular Haskell compiler, GHC \citep{HistoryOfHaskell}, is able to
compile and run parallel Haskell programs `out of the box'. This ability is
limited to the \emph{shared memory processors}, also known as symmetric
multiprocessors (SMP), that are nearly ubiquitous with the rise of multi-core
architectures in modern CPUs. The GHC project provides several ways to utilise
parallel-capable machines.

The first method is through the \<par\> and \<seq\>\footnote{While
\texttt{seq} was introduced into Haskell for Haskell '98 \citep{Haskell98Book}
it was used for many years before that \citep{HistoryOfHaskell}. One of the
earliest descriptions was by Hughes who introduced a \texttt{synch} combinator
that performed the same function in 1983 \citep{hughes:thesis}.} combinators.
The \<seq\> combinator has the following type.

\begin{haskell}
seq &:: \hasalpha \to \hasbeta \to \hasbeta
\end{haskell}

An expression of the form \<seq a b\> first forces the evaluation of its first
argument to WHNF\footnote{For an expression to be in Weak Head Normal Form it
must be evaluated such that the outermost constructor is known, but not
necessarily any further.  To say that a function is in WHNF means that the
function is partially applied.} and then returns its second argument. This
allows a programmer to express sequential evaluation. It is essential to note
that \<seq\> $\bot$ \<b\> results in $\bot$.

It is important to realise that GHC's implementation of \<seq\> is
\emph{not} guaranteed to force the evaluation of its first argument
\emph{before} its second argument. The compiler may find an optimisation that
circumvents the sequencing created by the combinator. In order to provide a
combinator that \emph{does} provide this guarantee GHC provides the
\<pseq\>\footnote{The `p' in \texttt{pseq} stands for parallel.  The idea
being that parallel programs are more likely than sequential programs to
require that \texttt{seq} guarantees its behaviour.} combinator.

In the literature, the \<par\> combinator appears in one of two forms
\citep{HistoryOfHaskell, hughes:thesis}. In order to differentiate between the
two forms we will refer to the earlier version as the \emph{applicative} par, or
\<parAp\> and the more recent (and more common) version as \emph{Haskell}
\<par\>\footnote{The reason we will be referring to this as Haskell par is
because most users will know this version of the combinator from their use of
Haskell}.

Haskell \<par\> takes the same form as \<seq\> and has the same type
signature. The combinator takes two arguments, sparks off the first for
evaluation in parallel, and returns the second.

\begin{haskell}
par &:: \hasalpha \to \hasbeta \to \hasbeta \\
par a b &= b
\end{haskell}

The applicative \<parAp\> expresses a function application whose parameter has been
sparked off to be evaluated in parallel. Semantically, this means that the
applicative par has the following form

\begin{haskell}
parAp &:: (\hasalpha \to \hasbeta) \to \hasalpha \to \hasbeta \\
parAp f x &= f x
\end{haskell}

% TODO maybe find a place to show this
%The strictness of the function being applied can have a huge effect on the
%parallelism from this combinator. In fact, we will see in section
%\ref{sec:experiments} that in same cases, the compilation of the
%function being applied can have large effects on \verb=parAp='s use.

Interestingly, the version of \<par\> that an implementation chooses does
not change the expressiveness. Each version of \<par\> can actually be
defined in terms of the other. Defining the applicative par in terms of Haskell
par gives us

\begin{haskell}
parAp f x = par x (f x)
\end{haskell}

In order to define Haskell par in terms of applicative par we must use the
\<k\> combinator

\begin{haskell}
k x y &= x \\
\quad&\quad \\
par x y &= parAp (k y) x
\end{haskell}

The benefit of \<parAp\> is that you get the sharing of the parallel computation
without needing to give a name to a subexpression. For example, the following use of
\<parAp\> is very typical.

\begin{haskell}
parAp f (g 10)
\end{haskell}

This does as you would expect: evaluate \<g 10\> and share that with
\<f\>. In order to achieve the same effect using Haskell \<par\> we must
give \<g 0\> a name.

\begin{haskell}
\hslet{x = g 10}{%
       par x (f x)}
\end{haskell}


While language implementors may have strong preferences for one over the other,
there are a few good arguments for the use of Haskell \<par\>. Haskell
\<par\> is the simpler of the two versions, using it as an infix combinator
makes it easy to spark an arbitrary number of expressions easily,
\<a `par` b `par` c\>, and the use-case for applicative \<par\> can be easily
expressed using Haskell \<par\>, as shown above.

\paragraph{Writing a program with \texttt{par} and \texttt{seq}}

Now that we have our essential combinators we are able to define a parallel
algorithm. One of the big sellers of functional programming is the wonderfully
concise \<quicksort\> definition

\pagebreak

\begin{haskell}
quicksort &:: (Ord \hasalpha) \Rightarrow [\hasalpha] \to [\hasalpha]\\
quicksort (x:xs) &= lesser \hsapp x:greater
            \hswhere{lesser    &= quicksort [y \mid y \hsfrom xs, y \leq x] \\
                     greater &= quicksort [y \mid y \hsfrom xs, y > x]} \\
quicksort \_ &= []
\end{haskell}

The obvious way to parallelise this algorithm is to ensure that each of the two
recursive calls can be executed in parallel. This is a common form of
parallelism known as `divide and conquer' \citep{skeletons}.  This can be done
by changing the second line to

\begin{haskell}
quicksort (x:xs) = greater `par` (lesser \hsapp x:greater)
\end{haskell}

The issue with the above is that while the left-hand side is sparked off in
parallel, it will only be evaluated to WHNF if the spark catches. This will
result in only the first \<cons\> of the list. The rest of the list will
only be evaluated if/when \<greater\> is needed\footnote{Because of
laziness it is possible that \texttt{greater} will never be needed. An
example would be if only the head of the sorted list is requested.}. This
fails to exploit all of the parallelism we desire and highlights the sometimes
conflicting nature of parallelism and laziness.

    In order to help us attain the parallelism we are aiming for, we can
introduce a function \<force\> that ensures that its parameters are evaluated
fully. As found in the textbook ``Real World Haskell'' \citep{realWorld}

\begin{haskell}
force &:: [\hasalpha] \to ()\\
force list &= force' list `pseq` ()
\hswhere{force' (\_:xs) &= force' xs \\
         force' []      &= ()
        }
\end{haskell}

This function takes a list and enforces spine-strictness. As long as the list is
not an infinite structure, the use of this function is safe. With this function
in hand we could adapt our basic parallel \<quicksort\> into a better performing one.

An interesting point is that this definition of \<force\> can be much simpler

\pagebreak

\begin{haskell}
force &:: [\hasalpha] \to () \\
force (\_:xs) &= force xs\\
force []           &= ()
\end{haskell}

Because the function is fully saturated, the recursive call will continue
evaluating through the entirety of the list. This only goes to show that even
experts sometimes misunderstand when combinators like \<seq\> and \<pseq\> are
needed.

\begin{haskell}
parQuicksort &:: (Ord \hasalpha) \Rightarrow [\hasalpha] \to [\hasalpha]\\
parQuicksort (x:xs) &= force greater `par` (force lesser `pseq` (lesser ++ x:greater))
\hswhere{lesser &= parQuicksort [y \mid y \hsfrom xs, y \leq x]\\
         greater &= parQuicksort [y \mid y \hsfrom xs, y > x]
        }
parQuicksort \_ = []
\end{haskell}

Notice that there were a few more changes than just calling \<force\> with
the parallel spark. By also forcing the evaluation of \<lesser\> before
appending the two lists we ensure that both \<greater\> and \<lesser\>
are constructed completely. While this version of a parallel \<quicksort\> does
execute its recursive calls in parallel, it has come at a cost. First, the
resulting list is no longer lazy. Using this function for determining the head
of the resulting list would result in the entire list being computed. While
the loss of laziness can sometimes be a worthwhile trade-off (particularly
if the entire list will be required anyway) for an increase in speed, the
second major cost is that this parallel \<quicksort\> does not result in a faster
program!

    Despite the added functions to ensure that laziness did not get in the way
of our desired parallelism, the parallel sorting was actually slower than the
sequential sort. Running a parallel \<quicksort\> on a two core machine, O'Sullivan
found that this algorithm actually resulted in a $23\%$ decrease in
performance \citep{realWorld}. The reason for the slowdown comes up often in the
design of parallel algorithms: there is a minimum amount of work a thread must
perform in order to make the expense of sparking off the thread worthwhile. This
issue highlights what is known as granularity \citep{dutchBook}. While the sparking
of threads is relatively cheap in a system such as GHC, there is still a cost,
and if the amount of work a thread performs is very little, the cost may not be
worth it.

    One way of tackling this issue is by introducing the notion of \emph{depth}
to the function. An additional parameter can be be added to the function that
acts as a count for the depth.

\begin{haskell}
parQuicksort (x:xs) depth = \hsif{depth \leq 0\\}{%
                                  quicksort (x:xs)}{%
                                    else \dots}
\end{haskell}

    In this version we check to see if the desired maximum depth has been
reached and, if so, then the recursive call is to the standard sequential sorting
algorithm. If the max depth is not reached then we use the body of the
parQuicksort defined above, with the depth argument to each recursive call being
\<(depth - 1)\>. The desired maximum depth is determined by the value given as
the depth argument at the top-level call of the function. This method of
controlling the granularity is a useful one and allows for easy experimentation
on the maximum depth for the problem at hand. The downside is that it
contributes yet another concern for the programmer to worry about.


Managing the granularity of a parallel program is one of the bigger challenges
facing parallel functional programming \citep{SPJ:PIFPL}. Whether decided by
the programmer (with annotations) or by the compiler implicitly, the question
of ``is this task worth it?'' will always come up when deciding what tasks
should be sparked.

\subsection{Explicit Parallelism with the \texttt{Par} Monad}

One of the main benefits of using \<par\> and \<seq\> is that they provide a
simple interface for expressing parallel computation without the need to worry
about concurrency issues such as task/thread creation and synchronisation. The
communication is via the same mechanism that allows for lazy evaluation
\citep{SPJ:PIFPL}. However, laziness can be difficult to reason about and
gaining the maximum benefit from parallelism can often require the
\emph{forcing} of values beyond WHNF, much like what was done with the
\<force\> function in the previous section.  This motivated another approach to
deterministic parallelism: the \<Par\> Monad \citep{marlow2011monad}.

The \<Par\> Monad makes the following trade-off: for the cost of making the
dependencies between computations explicit and prohibiting the use of shared
lazy structures, users gain predictable performance and the ability to reason
about when evaluation takes place. The library provides an API for dataflow
parallelism in Haskell.

Internally the \<Par\> Monad is implemented using I-Structures
\citep{Arvind:IStructures} in the form of \<IVar\>s, which provide the
mechanism for communication between tasks. \<IVar\> are mutable cells with
write-once semantics.  The disciplined use of I-Structures in the \<Par\> Monad
are what ensure deterministic parallelism.

To illustrate the data-flow nature of the \<Par\> Monad we can use its API to
write a function that computes two calls to \<fib\> in parallel (this code is
adapted from \citet[pg. 59]{marlowBook}).

\begin{haskell}
twoFib m n &= runPar \$ \hsdo{%
    i \hsfrom new \\
    j \hsfrom new \\
    fork (put i (fib m))\\
    fork (put j (fib n))\\
    a \hsfrom get i\\
    b \hsfrom get j\\
    return (a, b)
    }
\end{haskell}

The \<Par\> Monad provides us with an API that is quite explicit about the
data-flow of the program. This allows the library to schedule the threads
as appropriate. The function \<put\> takes an \<IVar\> and a value that
can be \emph{forced} and write the value to the \<IVar\>. By being
so explicit about the structure of the program the library allows for
intuitive reasoning about the parallel performance.

%\footnote{We found that running a similar experiment on a 4-core machine
%did not improve the results by much.}

\subsection{Semi-Implicit Parallelism with Strategies}

This section will look at Parallel Strategies to illustrate how a programmer
can write the algorithm they want (for the most part) and then use strategies
to specify how that algorithm should be parallelised. Strategies become central
to our technique and are mentioned throughout this thesis, as they are used as
the method of introducing parallelism into a program.

Strategies came in two waves; the first wave was introduced by
\citet{strategies} in "Algorithm \(+\) Strategy \(=\) Parallelism". This
incarnation of strategies is simple to understand and easy to use effectively.
The second wave was an adaptation of the idea that addressed an issue caused by
the garbage collector, but the main \emph{idea} remains the same
\citep{marlow2010seq}.

The type declaration for a Strategy is

\begin{haskell}
\hskwd{type} Strategy \hasalpha = \hasalpha \to ()
\end{haskell}

The key point to take away from this is that strategies do not, and can not,
affect the \emph{value} of the computation.

It is possible to define strategies that do not introduce parallelism, but
instead ensure that evaluation is carried out to some degree.  For example, the
strategy for doing nothing

\begin{haskell}
r0 :: Strategy a\\
r0 \_ = ()
\end{haskell}

And for evaluating the argument to WHNF

\begin{haskell}
rwhnf   &:: Strategy \hasalpha\\
rwhnf x &= x `seq` ()
\end{haskell}

These strategies are not often used on their own, but are used in conjunction
with other strategies to achieve a goal. For example, applying a strategy to
each element of a list can be expressed as

\begin{haskell}
seqList &:: Strategy \hasalpha \to Strategy [\hasalpha]\\
seqList strat [] &= ()\\
seqLIst strat (x:xs) &= strat x `seq` (seqList strat xs)\\
\quad&\quad \\
parList &:: Strategy \hasalpha \to Strategy [\hasalpha] \\
parList strat [] &= () \\
parList strat (x:xs) &= strat x `par` (parList strat xs)
\end{haskell}

Both of these functions are common strategies for working on lists. In the first
case, \<seqList\> elements in the list are \emph{sequentially} evaluated
using \<strat\> In \<parList\> each element is evaluated in parallel
using \<strat\> this gives no guarantee of the evaluation order, only that
the sparks will point to each element of the list.

In order to use a strategy, the function \<using\> was defined; taking an
expression and a strategy, it bridges the gap between the specified algorithm
and the desired strategy.

\begin{haskell}
using &:: \hasalpha \to Strategy \hasalpha \to \hasalpha\\
using x s &= s x `seq` x
\end{haskell}

This allows us to define \<parMap\> as

\begin{haskell}
parMap &:: Strategy \hasbeta \to (\hasalpha \to \hasbeta) \to [\hasalpha] \to [\hasbeta]\\
parMap strat f xs &= map f xs `using` parList strat
\end{haskell}

With this we can evaluate all the elements of a list to whatever level the
first argument specifies.

\subsection{Semi-Implicit Parallelism with Skeletons}

There are many cases where the \emph{structure} of two programs are the same
even though they are computing different results. As mentioned above,
\<quicksort\> shares a structure with many algorithms known as `divide and
conquer'. Algorithmic skeletons allow for the re-use of \emph{structural}
parallelism between programs \citep{skeletons}.

To a functional programmer, skeletons are higher-order functions that are
passed the computation specific actions of a program and ensure that the
actions are parallelised according to the pre-defined structural parallelism.
\citet{skeletons} introduce skeletons for the \emph{divide and conquer},
\emph{farm} (like a generic \<parMap\>), \emph{pipe}, and \emph{reduce} (like a generic
parallel \<fold\>) patterns. The \emph{divide and conquer} skeleton is defined as follows:

\begin{haskell}
dc triv solve divide conq x \hsbody{%
    &\mid triv x       &=\ solve x \\
    &\mid otherwise    &=\ conq (solveAll triv solve divide conq (divide x))
    }\hswhere{%
    solveAll xs &= \hsbody{Collection.map (dc triv solve divide conq) xs `using` parColl all}
    }
\end{haskell}

The process is rather simple, \<dc\> takes a predicate \<triv\> that determines
whether the input value, \<x\>, is trivial. If so, we use the function
\<solve\> for the trivial case. When \<x\> is not trivial we must split the
problem into smaller chunks using \<divide\>. With the problem divided, we use
\<solveAll\> which recursively calls \<dc\> in parallel over all of the divided
parts of the problem. The parallelism is introduced using a Strategy in
\<solveAll\>. Lastly, \<conq\> is used to combine the results of the
sub-problems.

There are two restrictions on the use of \<dc\>. The first is that the result be a member of
the class \<Evaluable\> which has three member functions, \<none\>, \<outer\>,
and \<all\>. These functions correspond to strategies with various degrees of
evaluation: \<none\> performs no evaluation of the value, \<outer\> evaluates
to WHNF, and \<all\> evaluates to normal form. The second is that the container type
must be a member of \<Coll\> which provides a uniform interface for \<map\>ing
over its elements.

Skeletons provide a powerful tool when working with problems that are known to
be parallelisable. If a programmer can recognise which skeleton is appropriate
for their problem domain they can focus on the aspects that are specific to
their problem and let the skeleton manage the parallelism \citep{skeletons}.


\subsection{Semi-Implicit Parallelism via Rewriting}


Some recent work has shown promising results with the use of rewriting. The
programmer uses pre-selected functions (which is what makes these techniques
semi-implicit) and the compiler uses pre-defined rewrite rules to convert the
high-level function to low level parallel code \citep{SteuwerPortable}. This
technique frees the programmer from concerning themselves with the low-level
optimisation techniques, which in the case of GPU programming are often vendor
specific. The compiler writers provide different sets of rewrite rules for each
of the provided high-level functions. The compiler then searches the space of
rewrite rules, allowing it to tune the performance of each function to a
specific GPU. Because this search is done offline, the programmer can freely
use any of the provided parallel functions knowing that whatever GPU the
program runs on, an efficient sequence of rewrite rules is known to go from the
high-level source to the target GPU.

This technique, like ours, can be seen as applying search-based techniques to
the problem of knowing when parallelism can be accomplished. The benefit of
their method is that it is restricted to a finite set of pre-determined
parallel functions.

Another rewriting approach uses a \emph{proof} of the program in separation
logic, to automatically parallelise the program where it is safe, having the
safety guaranteed by the provided proof \citep{hurlinProof}. The proof can be
transformed as the program is transformed providing assurance that the
resulting program has the same semantic behaviour. This technique does save the
programmer from having to manually parallelise the code, at the cost of
requiring that the algorithm has a separation proof. In any instance where such
a proof is required for any other reason, this technique can be seen as `free'.
Otherwise the cost of proving separation could be limit the viability of this
method.


\subsection{Implicit Parallelism}

While explicit parallelism has many advantages and can show great performance
increases, many desire the ability to express a functional program
\emph{without} having to specify where parallelism can take place. This idea,
that parallelism can be achieved without programmer intervention, is known as
implicit parallelism. For non-strict languages it is not possible to
parallelise all possible expressions because doing so may introduce
non-termination that would not have occurred otherwise. \emph{Strictness
Analysis} is usually used to determine which expressions are \emph{needed} and
therefore safe to parallelise.  Strictness analysis will be covered in far more
detail in Chapter \ref{chap:discovery}, but we will provide a high-level
overview here to provide some context.  

\subsection*{Strictness Analysis for Implicit Parallelism}

Laziness can work against our desire to exploit possible parallelism in
functional programs. Because of this, researchers have discovered that using
static analysis at compile time in order to discover strict portions of a
program can yield promising results. This analysis has been named
\emph{strictness analysis} \citep{ritabook, SPJ:PIFPL}. A trivial example is
that of the addition of two expressions, such as the fibonacci sequence.

\begin{haskell}
nfib n \hsbody{%
            &\mid n \leq 0   &= 0 \\
            &\mid n \equiv 1    &= 1 \\
            &\mid otherwise  &= nfib (n-1) + nfib (n-2)
            }
\end{haskell}

Because the \<(+)\> function is strict in both its arguments we know that
both recursive calls to \<nfib\> will be required. This fact (that we
\emph{know} that the arguments to a function will be required by the function)
is what enables us to exploit the parallelism that is inherent in the program.

Strictness analysis has been deemed \emph{conservative} parallelism
\citep{SPJ:PIFPL}. This is due to the idea that sparking an expression that
will definitely be required by the program is not seen as risky. However, keep
in mind that while it is known as \emph{conservative} parallelism it does not
mean that there is any shortage of parallel tasks, it only refers to the fact
that no \emph{speculative} parallelism will be undertaken.

One of the issues with standard strictness analysis when used for implicit
parallelism is that it does not take into account the \emph{context} in which
an expression takes place. Take the standard recursive definition of
\<append\>:

\begin{haskell}
append [] ys &= ys \\
append (x:xs) ys &= x : append xs ys
\end{haskell}

In general the first argument is needed, but only to the outermost constructor.
However, when \<append\> is used in certain contexts, like returning its result
to a function that requires a finite list:

\begin{haskell}
length (append xs ys)
\end{haskell}

Then \<append\> actually requires both of its arguments to be finite. This
notion of context has been dealt with in two main ways in the literature:
\emph{Evaluation Transformers} and \emph{Projection-Based Strictness Analysis}.
We will examine Evaluation Transformers in Chapter \ref{chap:derivation}, and
Projection-Based Strictness Analysis in Chapter \ref{chap:discovery}. In short
both of these techniques allow the compiler or runtime system to take advantage
of the information that an expression's context provides regarding how defined
a value must be.

Many of the attempts at automatic parallelisation used the technique pioneered
by \citet{hogen1992automatic}, which uses strictness analysis to identify safe
parallelism and \emph{evaluation transformers} \citep{burn1987evaluation} to
good effect. The main setback of this popular approach is that there is no
method for \emph{refining} the initial placement of parallelism. Strictness
analysis is not equipped to determine the \emph{benefit} of evaluating an
expression in parallel, only the \emph{safety}. We refer to this problem as the
\emph{granularity problem} throughout the thesis.


\subsection*{Feedback-Directed Implicit Parallelism}

\citet{feedbackImplicit} proposed using \emph{iterative feedback} in order to
assist the compiler in determining which expressions are expensive enough to
warrant evaluation in parallel. Their approach forgoes the use of strictness
analysis to place parallelism into a program. Instead they utilise runtime
profiling to measure the lifetime of thunks in the heap and the relationship
between the thunks. They can then study dependency graph between thunks
and determine which thunks are worthwhile for parallelism.

Because each thunk has a unique allocation site, they are able to provide an
estimate for the `total work' performed by an allocation site. They define this
measure as $t/a$ where $t$ is the amount of time that \emph{at least} one thunk
from that allocation site is being evaluated, and $a$ is the number of thunks
allocated during that time.  This prevents an allocation site from being deemed
worthwhile because of a long lifetime but in reality it creates numerous thunks
each requiring a small amount of work, flooding the runtime system with
small-grained tasks \citep[Section 3]{feedbackImplicit}.
