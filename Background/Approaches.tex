When looking at parallel programming, it is important to make the distinction
between concurrency and parallelism.  Concurrency embodies the idea of multiple
workers (threads, computers, agents, etc.) making progress on independent
tasks. A standard example for concurrency is a modern web-server. Each
connection to the web-server can be thought of as an independent sub-task of
the program. The web-server does not have to be run on a multi-core machine for
the concurrency to take place. A single-core machine is capable of running
concurrent threads through scheduling and context switching.

Parallelism describes the simultaneous execution of tasks with the purpose of
achieving a gain in performance. Tasks that can be easily divided into
independent sub-tasks can easily be made into parallel algorithms. For example,
if one wanted to compute the monthly average temperature for a given year, each
month could be computed independently.  If enough of these independent
computations happen simultaneously there can be a substantial improvement in
the program's wall-clock speed. Ideally, the increase in performance would
scale at the same rate as the available parallel machinery (2$x$ performance
with two processors, 5$x$ performance with 5 processors). Unfortunately, there
are some unavoidable factors that prevent this ideal from being realised
\citep{hughes:thesis,HistoryOfHaskell,PFPAnIntro}. The most basic fact
preventing this ideal is that the executing machinery (whether virtual or
physical) will necessarily introduce some overhead in the generation and
management of parallel threads of execution \citep{PeytonJones:IFL}. Beyond
that, it is unusual for a program to be perfectly parallel except in trivial
cases. Most parallel programs exhibit non-uniform parallelism and complex data
dependencies. This results in programs where parallel threads vary greatly in
their processing time and contain threads of execution that will depend on the
results of other threads. This results in threads having to wait for the result
of another thread before commencing (which is known as blocking).

\subsection{Haskell}

Haskell is a lazy functional language benefiting from constant development
since its inception in 1987\footnote{1987 was when the academic community
decided that an `standard' language was needed to unify the study of lazy
functional programming\citep{HistoryOfHaskell}, however, the first Haskell
report was not published until 1990 \citep{Haskell98Book}}
\citep{HistoryOfHaskell, Haskell98Book}. Haskell as defined by the Haskell
Report \citep{Haskell98Book} does not have parallelism as part of the language.
However, functional programming has a long history of parallel implementations
and Haskell is no different in this regard. Even the early implementations of
Haskell had facilities for parallel computation.

\paragraph{Haskell Parallelism}

The Glasgow Haskell Compiler (GHC) has extensive parallelism and concurrency
support, this is part of what makes the compiler a popular implementation
\citep{HistoryOfHaskell}. While our eventual goal is to have access to
compilers that take advantage of the implicit parallelism in our programs, it
is useful to understand the tools and libraries that enable programmers to use
\emph{explicit} parallelism in their Haskell programs. Some of these techniques
are well-established and have decades of use and experience to draw from. Other
techniques are more modern and provide new methods for Haskell programmers to
use parallelism. Often, these newer methods are an `answer' to problems or
pitfalls with the older techniques.

\subsection{Explicit Parallelism with \texttt{par} and \texttt{seq}}

The most popular Haskell compiler, GHC \citep{HistoryOfHaskell}, is able to
compile and run parallel Haskell programs `out of the box'. This ability is
limited to the \emph{shared memory processors}, also known as symmetric
multiprocessors (SMP), that are nearly ubiquitous with the rise of multi-core
architectures in modern CPUs. The GHC project provides several ways to utilise
parallel-capable machines.

The first method is through the \<par\> and \<seq\>\footnote{While
\texttt{seq} was introduced into Haskell for Haskell '98 \citep{Haskell98Book}
it was used for many years before that \citep{HistoryOfHaskell}. One of the
earliest descriptions was by Hughes who introduced a \texttt{synch} combinator
that performed the same function in 1983 \citep{hughes:thesis}.} combinators.
The \<seq\> combinator has the following type.

\begin{haskell}
seq &::& \hasalpha \to \hasbeta \to \hasbeta
\end{haskell}

An expression of the form \<seq a b\> first forces the evaluation of its
first argument to WHNF\footnote{For an expression to be in Weak Head Normal
Form it must be evaluated enough such that the outermost constructor is known.
To say that a function is in WHNF means that the function is partially
applied.} and then returns its second argument. This allows a programmer to
express sequential evaluation. It is important to note that \<seq\> $\bot$
\<b\> results in $\bot$.

It is important to realise that GHC's implementation of \<seq\> is
\emph{not} guaranteed to force the evaluation of its first argument
\emph{before} its second argument. The compiler may find an optimisation that
circumvents the sequencing created by the combinator. In order to provide a
combinator that \emph{does} provide this guarantee GHC provides the
\<pseq\>\footnote{The `p' in \texttt{pseq} stands for parallel.  The idea
being that parallel programs are more likely than sequential programs to
require that \texttt{seq} guarantees its behavior.} combinator.

In the literature, the \<par\> combinator appears in one of two forms
\citep{HistoryOfHaskell, hughes:thesis}. In order to differentiate between the
two forms we will refer to the earlier version as the \emph{applicative} par, or
\<parAp\> and the more recent (and more common) version as \emph{Haskell}
\<par\>\footnote{The reason we will be referring to this as Haskell par is
because most users will know this version of the combinator from their use of
Haskell}.

Haskell \<par\> takes the same form as \<seq\> and has the same type
signature. The combinator takes two arguments, sparks off the first for
evaluation in parallel, and returns the second.

\begin{haskell}
par &::& \hasalpha \to \hasbeta \to \hasbeta \\
par a b &=& b
\end{haskell}

The applicative \<parAp\> expresses a function application whose parameter has been
sparked off to be evaluated in parallel. Semantically, this means that the
applicative par has the following form

\begin{haskell}
parAp &::& (\hasalpha \to \hasbeta) \to \hasalpha \to \hasbeta \\
parAp f x &=& f x
\end{haskell}

% TODO maybe find a place to show this
%The strictness of the function being applied can have a huge effect on the
%parallelism from this combinator. In fact, we will see in section
%\ref{sec:experiments} that in same cases, the compilation of the
%function being applied can have large effects on \verb=parAp='s use.

Interestingly, the version of \<par\> that an implementation chooses does
not change the expressiveness. Each version of \<par\> can actually be
defined in terms of the other. Defining the applicative par in terms of Haskell
par gives us

\begin{haskell}
parAp f x = par x (f x)
\end{haskell}

In order to define Haskell par in terms of applicative par we must use the
\<k\> combinator

\begin{haskell}
k x y &=& x \\
\quad&&\quad \\
par x y &=& parAp (k y) x
\end{haskell}

The benefit of \<parAp\> is that you get the sharing of the parallel computation
without needing to give a name to a subexpression. For example, the following use of
\<parAp\> is very typical.

\begin{haskell}
parAp f (g 10)
\end{haskell}

This does as you would expect, evaluate \<g 10\> and share that with
\<f\>. In order to achieve the same effect using Haskell \<par\> we must
give \<g 0\> a name.

\begin{haskell}
\hslet{x = g 10}{%
       par x (f x)}
\end{haskell}


While language implementors may have strong preferences for one over the other,
there are a few good arguments for the use of Haskell \<par\> Haskell
\<par\> is the simpler of the two versions, using it as an infix combinator
makes it easy to spark an arbitrary number of expressions easily,
\<a `par` b `par` c\>, and the use-case for applicative \<par\> can be easily
expressed using Haskell \<par\>, as shown above.

\todoinline{It may be nice to instrument and profile the quicksort example that comes
    next. Since we have the space to do it, showing how we get a better performing
    algorithm and actually showing the speedups would be insightful.}
\paragraph{Writing a program with \texttt{par} and \texttt{seq}}

Now that we have our essential combinators we are able to define a parallel
algorithm. One of the big sellers of functional programming is the wonderfully
concise Quicksort definition

\begin{haskell}
quicksort &:: (Ord \hasalpha) \Rightarrow [\hasalpha] \to [\hasalpha]\\
quicksort (x:xs) &= less \hsapp x:greater
            \hswhere{less    &= quicksort [y \mid y \hsfrom xs, y \leq x] \\
                     greater &= quicksort [y \mid y \hsfrom xs, y > x]} \\
quicksort \_ &= []
\end{haskell}

The obvious way to parallelise this algorithm is to ensure that each of the two
recursive calls can be executed in parallel, this is a common form of
paralellism known as `divide and conquer' \tocite{Either Hammond or something
earlier}. This can be done by changing the second line to

\begin{haskell}
quicksort (x:xs) = greater `par` (less \hsapp x:greater)
\end{haskell}

The issue with the above is that while the left-hand side is sparked off in
parallel, it will only be evaluated to WHNF if the spark catches. This will
result in only the first \<cons\> of the list. The rest of the list will
only be evaluated if/when \<greater\> is needed\footnote{Because of
laziness it is possible that \texttt{greater} will never be needed. An
example would be if only the head of the sorted list is requested.}. This
fails to exploit all of the parallelism we desire and highlights the sometimes
conflicting nature of parallelism and laziness.

    In order to help us attain the parallelism we are aiming for, we can
introduce a function \<force\> that ensures that its parameters are evaluated
fully. As found in the textbook ``Real World Haskell'' \citep{realWorld}

\begin{haskell}
force &:: [\hasalpha] \to ()\\
force list &= force' list `pseq` ()
\hswhere{force' (\_:xs) &= force' xs \\
         force' []      &= 1
        }
\end{haskell}

This function takes a list and enforces spine-strictness. As long as the list is
not an infinite structure the use of this function is safe. With this function
in hand we could adapt our basic parallel Quicksort into a better performing one.

An interesting point is that this definition of \<force\> can be much simpler

\begin{haskell}
force &:: [\hasalpha] \to () \\
force (\_:xs) &= force xs\\
force []           &= ()
\end{haskell}

Because the function is fully saturated, the recursive call will continue
evaluating through the entirety of the list. This only goes to show that even
experts sometimes misunderstand when combinators like \<seq\> and \<pseq\> are
needed.

\begin{haskell}
parQuicksort &:: (Ord \hasalpha) \Rightarrow [\hasalpha] \to [\hasalpha]\\
parQuicksort (x:xs) &= force greater `par` (force lesser `pseq` (less ++ x:greater))
\hswhere{less &= parQuicksort [y \mid y \hsfrom xs, y \leq x]\\
         greater &= parQuicksort [y \mid y \hsfrom xs, y > x]
        }
parQuicksort \_ = []
\end{haskell}

Notice that there were a few more changes than just calling \<force\> with
the parallel spark. By also forcing the evaluation of \<lesser\> before
appending the two lists we ensure that both \<greater\> and \<lesser\>
are constructed completely. While this version of a parallel Quicksort does
execute its recursive calls in parallel it has come at a cost. First, the
resulting list is no longer lazy. Using this function for determining the head
of the resulting list would result in the entire list being computed. While
the loss of laziness can sometimes be a worthwhile trade-off (particularly
if the entire list will be required anyway) for an increase in speed, the
second major cost is that this parallel Quicksort does not result in a faster
program!

    Despite the added functions to ensure that laziness did not get in the way
of our desired parallelism, the parallel sorting was actually slower than the
sequential sort. Running a parallel Quicksort on a two core machine, O'Sullivan
found that this algorithm actually resulted in a $23\%$ decrease in
performance \citep{realWorld}. The reason for the slowdown comes up often in the
design of parallel algorithms: There is a minimum amount of work a thread must
perform in order to make the expense of sparking off the thread worthwhile. This
issue highlights what is known as granularity \citep{dutchBook}. While the sparking
of threads is relatively cheap in a system such as GHC, there is still a cost,
and if the amount of work a thread performs is very little, the cost may not be
worth it.

    One way of tackling this issue is by introducing the notion of \emph{depth}
to the function. An additional parameter can be be added to the function that
acts as a count for the depth.

\begin{haskell}
parQuicksort (x:xs) depth = \hsif{depth \leq 0\\}{%
                                  quicksort (x:xs)}{%
                                    else \dots}
\end{haskell}

    In this version we check to see if the desired maximum depth has been
reached and if so then the recursive call is to the standard sequential sorting
algorithm. If the max depth is not reached then we use the body of the
parQuicksort defined above, with the depth argument to each recursive call being
\<depth - 1\> The desired maximum depth is determined by the value given as
the depth argument at the top-level call of the function. This method of
controlling the granularity is a useful one and allows for easy experimentation
on the maximum depth for the problem at hand. The downside is that it
contributes yet another concern for the programmer to worry about.


Managing the granularity of a parallel program is one of the bigger challenges
facing parallel functional programming \citep{SPJ:PIFPL}. Whether decided by
the programmer (with annotations) or by the compiler implicitly, the question
of ``is this task worth it?'' will always come up when deciding what tasks
should be sparked.

\todoinline{To get a really fast quicksort, we'd need to actually implement
    a `real' in place quicksort using something like the ST monad. This would
    be a good spot to discuss the fast that sometimes it's the algorithm that
    is the bottleneck, and not the lack of implicit parallelism.}

\subsection{Explicit Parallelism with the \texttt{Par} Monad}

One of the main benefits of using \<par\> and \<seq\> is that they
provide a simple interface for expressing parallel computation without
the need to worry about concurrency issues such as task/thread creation
and synchronisation. The communication is via the same mechanism that
allows for lazy evaluation \citep{SPJ:PIFPL}. However, laziness can
be difficult to reason about and gaining the maximum benefit from parallelism
can often require the \emph{forcing} of values beyond WHNF. \todo{Reference the forcing in quicksort}
This motivated another approach to deterministic parallelism, the \<Par\>
Monad \citep{marlow2011monad}.

The \<Par\> Monad makes the following trade-off: For the cost of making the
dependencies between computations explicit and prohibiting the use of shared
lazy structures, users gain predictable performance and the ability to reason
about when evaluation takes place. The library provides an API for dataflow
parallelism in Haskell.

Internally the \<Par\> Monad is implemented using I-Structures
\citep{Arvind:IStructures}, which provide the mechanism for communication
between tasks. I-Structures are mutable cells with write-once semantics.
The disciplined use of I-Structures in the \<Par\> Monad are what ensure
deterministic parallelism.


%\footnote{We found that running a similar experiment on a 4-core machine
%did not improve the results by much.}

 \subsection{Implicit Parallelism}
   While explicit parallelism has many advantages and can show great performance
increases, many desire the ability to express a functional program
\emph{without} having to specify where parallelism can take place. This idea,
that parallelism can be achieved without programmer intervention, is known as
implicit parallelism.

  \paragraph{Strictness Analysis}
    Laziness can work against our desire to exploit possible parallelism in
functional programs. Because of this, researchers have discovered that using
static analysis at compile time in order to discover strict portions of a
program can yield promising results. This analysis has been named
\emph{strictness analysis} \citep{ritabook, SPJ:PIFPL}. A trivial example is that
of the addition of two expressions, such as the fibonacci sequence.

\begin{haskell}
nfib n \hsbody{%
            &\mid n \leq 0   &= 0 \\
            &\mid n \equiv 1    &= 1 \\
            &\mid otherwise  &= nfib (n-1) + nfib (n-2)
            }
\end{haskell}

Because the \<(+)\> function is strict in both its arguments we know that
both recursive calls to \<nfib\> will be required. This fact, that we
\emph{know} that the arguments to a function will be required by the function,
is what enables us to exploit the parallelism that is inherent in the program.

    Strictness analysis has
been deemed \emph{conservative} parallelism \citep{SPJ:PIFPL}. This is due to
the idea that sparking an expression that will definitely be required by the
program is not seen as risky. While for the most part this is true, in reality
it depends on how the sparking is handled by the runtime.

    There are also techniques that involve using a program's statistics
    gathered at runtime to better predict where parallelism can be exploited
    \citep{feedbackImplicit}.

\subsection{Semi-Implicit Parallelism with Strategies}
    This section will look at things like `Strategies' to illustrate how a
programmer can write the code they want to write (for the most part) and then
use meta-techniques to make that code parallel.

Strategies came in two waves; the first wave was introduced by Trinder et al. in
"Algorithm \(+\) Strategy \(=\) Parallelism". This incarnation of strategies is
simple to understand and easy to use effectively.

The type declaration for a Strategy is

\begin{haskell}
\hskwd{type} Strategy \hasalpha = \hasalpha \to ()
\end{haskell}

The key point to take away from this is that Strategies do not, and can not,
contribute to the end result of the computation.

It is possible, and useful, to define strategies that do not introduce
parallelism, but instead ensure that evaluation is carried out to some degree.
For example, the strategy for doing nothing

\begin{haskell}
r0 :: Strategy a\\
r0 \_ = ()
\end{haskell}

And for evaluating the argument to WHNF

\begin{haskell}
rwhnf   &:: Strategy \hasalpha\\
rwhnf x &= x `seq` ()
\end{haskell}

These strategies are not often used on their own, but are used in conjunction
with other strategies to achieve a goal. For example, applying a strategy to
each element of a list can be expressed as

\todoinline{fix pagebreak}

\begin{haskell}
seqList &:: Strategy \hasalpha \to Strategy [\hasalpha]\\
seqList strat [] &= ()\\
seqLIst strat (x:xs) &= strat x `seq` (seqList strat xs)\\
\quad&\quad \\
parList &:: Strategy \hasalpha \to Strategy [\hasalpha] \\
parList strat [] &= () \\
parList strat (x:xs) &= strat x `par` (parList strat xs)
\end{haskell}

Both of these functions are common strategies for working on lists. In the first
case, \<seqList\> elements in the list are \emph{sequentially} evaluated
using \<strat\> In \<parList\> each element is evaluated in parallel
using \<strat\> this gives no guarantee of the evaluation order, only that
the sparks will point to each element of the list.

In order to use a strategy, the function \<using\> was defined; taking an
expression and a strategy, it bridges the gap between the specified algorithm
and the desired strategy.

\begin{haskell}
using &:: \hasalpha \to Strategy \hasalpha \to \hasalpha\\
using x s &= s x `seq` x
\end{haskell}

This allows us to define \<parMap\> as

\begin{haskell}
parMap &:: Strategy \hasbeta \to (\hasalpha \to \hasbeta) \to [\hasalpha] \to [\hasbeta]\\
parMap strat f xs &= map f xs `using` parList strat
\end{haskell}

With this we could evaluate all the elements of a list to whatever level the
first argument specifies.

\subsection{Semi-Implicit Parallelism with Skeletons}

There are many cases where the \emph{structure} of two programs in the same
even though they are computing different results. As mentioned above,
\<quicksort\> shares a structure with many algorithms known as `divide and
conquer'. Algorithmic skeletons allow for reuse of \emph{structural} parallelism
between programs \tocite{Algorithmic Skeletons}.

Skeletons are higher-order functions that are passed the computation specific actions
of a program and ensure that the actions are parallelised according to the pre-defined
structural parallelism.

\todoinline{Give an example of a divide and conquer Skeleton}.

\subsection{Semi-Implicit Parallelism via Rewriting}


Some recent work has shown promising results with the use of rewriting. The
programmer uses pre-selected idioms (which is what makes these techniques
semi-implicit) and the compiler uses pre-defined rewrite rules to convert the
high-level idiom to low level parallel code \tocite{The ICFP 2015 paper
``Generating Performance Portable Code using Rewrite Rules: From High-level
Functional Expressions to High-Performance OpenCL Code''}. This technique frees
the programmer from concerning themselves with the low-level optimisation
techniques, which in the case of GPU programming are often vendor specific. The
compiler writers provide different sets of reqrite rules for each target
platform, providing portable parallelism for specific functional idioms.

Another rewriting approach uses a \emph{proof} of the program in seperation
logic \citep{hurlinProof}, to automatically parallelise the program where
it is safe, having the safety guaranteed by the provided proof. The proof can
be transformed as teh program is transformed providing assurance that the
resulting program has the same semantic behavior.
