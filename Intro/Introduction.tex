Advocates of purely functional programming languages often cite easy
parallelism as a major benefit of abandoning mutable state
\citep{hughes:thesis, SPJ:PIFPL}. This idea drove research into the theory and
implementation of compilers that take advantage of \emph{implicit parallelism}
in a functional program. Additionally, when research into implicit parallelism
was more common, the work was often based on novel architectures or distributed
systems, not commodity hardware \citep{GRIP, hammond2000research}. 

Despite this research effort the ultimate goal of writing a program in a
functional style, and having the compiler find the implicit parallelism, still
requires work.  We believe there are several reasons why previous work into
implicit parallelism has not achieved the results that researchers hoped for.
Chief amongst those reasons is that the static placement of parallel
annotations is not sufficient for creating well-performing parallel programs
\citep{hammond2000research, hogen1992automatic, tremblay1995impact,
feedbackImplicit}. This work explores one route to improvement: the compiler
can use runtime profile data to improve initial decisions about parallelism in
much the same way a programmer would manually tune a parallel program.

In the case of custom hardware, research was unable to keep up with huge
improvements in sequential hardware. Today most common desktop workstations are
parallel machines; this steers our motivation away from the full utilisation of
hardware. Many programmers today write sequential programs and run them on
parallel machines. We argue that even modest speedups are worthwhile if they
occur `for free'.

Historically Moore's law has often provided a `free lunch' for those looking to
run faster programs without the programmer expending any engineering effort.
Throughout the 1990s in particular, an effective way of having a faster x86
program was to wait for Intel\texttrademark{} to release its new line of
processors and run the program on your new CPU. Unfortunately, clock speeds
have reached a plateau and we no longer get speedups for free
\citep{sutter2005free}. Increased performance now comes from including
additional processor cores on modern CPUs.  This means that programmers have
been forced to write parallel and concurrent programs when looking for improved
wall-clock performance. Unfortunately, writing parallel and concurrent programs
involves managing complexity that is not present in single-threaded programs.
The goal of this work is to convince the reader that not all hope is lost. By
looking for the \emph{implicit parallelism} in programs that are written as
single-threaded programs we can achieve performance gains without programmer
effort.

Our work focuses on F-Lite: a pure, non-strict functional language that is
suitable as a core language of a compiler for a higher-level language like
Haskell \citep{naylor2010reduceron}. We have chosen to use a non-strict language
because of the lack of arbitrary side-effects \citep{whyFPmatters}, and many
years of work in the area of implicit parallelism \citep{hogen1992automatic,
PFPAnIntro, jones1993implicit} however we feel that many of our techniques
would transfer well to other language paradigms.

With the choice of a lazy functional language we introduce tension, the
evaluation order for lazy languages can be seen to be at odds with the goal of
only evaluating expressions when they are needed (which is an inherently
sequential evaluation order). For this reason we must utilise \emph{strictness
analysis} in order to statically determine what expression in a program are
definitely needed, allowing us to evaluate them in parallel. We note that even
eager languages would require some form of analysis because eager languages
tend to allow arbitrary side-effects, necessitating the careful introduction of
parallelism so as to avoid altering the order-dependent semantics of eager
programs.

In short, this work argues that static analysis is necessary but not sufficient
for the automatic exploitation of implicit parallelism. We argue that
\emph{some} form of runtime feedback is necessary to better utilise the
parallelism that is discovered via static analysis.

\section{Goals and Contributions of this Thesis}

The primary contribution of this thesis is to demonstrate that using search
based on dynamic execution of an automatically parallelised program is a robust
way to help diminish the \emph{granularity} problem that is difficult for
static analysis to overcome.

Our contributions can be seen as follows:

\begin{itemize}
    \item A method to automatically derive parallel strategies from a
            \emph{demand context}
    \item A novel use of heuristic search techniques in representing the
            possible parallelism in a program as a multi-dimensional search space
    \item The use of runtime profiles to \emph{disable} automatically introduced
            parallelism in a program
\end{itemize}

We show that for some programs, the combination of search and static analysis
can achieve speed-ups without the need for programmer intervention.

% Additionally we show that runtime profile data is not necessary to utilise feedback directed improvement: we show improvements based on overall runtime without `peeking' into the runtime system.
