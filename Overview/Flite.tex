The use of a small functional language as the internal representation of a
compiler is a common technique in functional compilers \cite{dutchBook,
PeytonJones:IFL, Augustsson:LazyMLCompiler, UHC}. By using a small core
language as an internal representation source language features are simply
syntactic sugar that is translated to a simpler but no less expressive
language. This provides compiler writers with a smaller surface area for
analysis and transformation. This has been used to great effect in the Glasgow
Haskell Compiler (GHC) which uses a small core language similar to ours
\cite{peyton2002secrets, jones1998transformation}. 

\subsection{Why be Lazy?}

Functional languages vary widely in their syntax, features, and type systems,
but almost all functional languages are either strict (eager) or non-strict
(and usually lazy) in their evaluation model. It is important to understand the
distinction between these two systems. Because functional languages can be
seen as enriched lambda calculi, we can study different evaluation
orders\footnote{Many texts describe them as \emph{evaluation strategies}.  We
use the term order to avoid confusion with parallel strategies, which are a
different concept that play a central role in this thesis.} by demonstrating
them on a simple lambda calculus. There are four main evaluation orders that
can be used with the lambda calculus:

    \begin{enumerate}
        \item Call-by-value
        \item Call-by-name
        \item Call-by-need
        \item Normal-order
    \end{enumerate}

The differences between the first 3 can be easily illustrated using the
following function definitions:

\begin{align*}
    sqr \ x \  &= \  x * x \\
    bot \ \_ \ &= \  \bot
\end{align*}

Now assume we want to evaluate the expressions \<sqr (5*5)\> and \<bot (5*5)\>.
We can manually reduce each of these expressions using each of the evaluation
orders.

\paragraph{Call-by-value}

\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{align*}
     &sqr\ (5*5) \\
  =\ &sqr\ 25 \\
  =\ &let\ x\ =\ 25\ in\ x * x \\
  =\ &25 * 25 \\
  =\ &625
\end{align*}
\begin{align*}
     &bot\ (5*5) \\
  =\ &bot\ 25 \\
  =\ &let\ x\ =\ 25\ in\ \bot \\
  =\ &\bot
\end{align*}
\end{multicols}
\caption{Call-by-value reduction}
\label{fig:call-by-value}
\end{figure}

Note that the argument to \<sqr\> and \<bot\> is evaluated \emph{before}
we enter the function's body. 

\pagebreak

\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{align*}
     &sqr\ (5*5) \\
  =\ &let\ x \  =\ (5*5)\ in\ x * x \\
  =\ &let\ x \  =\ (5*5)\ in\ (5*5) * x \\
  =\ &let\ x \  =\ (5*5)\ in\ 25 * x \\
  =\ &25 * (5*5) \\
  =\ &25 * 25 \\
  =\ &625
\end{align*}
\begin{align*}
     &bot\ (5*5) \\
  =\ &let\ x\ =\ 5*5\ in\ \bot \\
  =\ &\bot
\end{align*}
\end{multicols}
\caption{Call-by-name reduction}
\label{fig:call-by-name}
\end{figure}

\paragraph{Call-by-name} Here reduction delays the evaluation of a function's
argument until its use.  However, the result of evaluating a value is not
shared with other references to that value. This results in computing \<5*5\>
twice.

\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{align*}
     &sqr\ (5*5) \\
  =\ &let\ x\ =\ 5 * 5\ in\ x * x \\
  =\ &let\ x\ =\ 25\ in\ x * x \\
  =\ &25 * 25 \\
  =\ &625
\end{align*}
\begin{align*}
     &bot\ (5*5) \\
  =\ &let\ x\ =\ 5*5\ in\ \bot \\
  =\ &\bot
\end{align*}
\end{multicols}
\caption{Call-by-need reduction}
\label{fig:call-by-need}
\end{figure}

\paragraph{Call-by-need} This is designed to avoid the duplication of work that
is often a result of call-by-name evaluation. Notice that in this evaluation
\<(5*5)\> is bound to \<x\> as before but the result of computing the value of
\<x\> the first time \emph{updates} the binding. This is why call-by-need is
often referred to as call-by-name \emph{with sharing}, or \emph{lazy}.

An important point is that for languages without arbitrary side-effects call-by-name
and call-by-need are semantically equivalent. Call-by-need is an optimisation in the
\emph{implementation} of reduction.


\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{align*}
     &sqr\ (5*5) \\
  =\ &let\ x\ =\ 5 * 5\ in\ x * x \\
  =\ &let\ x\ =\ 5 * 5\ in\ 25 * x \\
  =\ &let\ x\ =\ 5 * 5\ in\ 25 * 25 \\
  =\ &625
\end{align*}
\begin{align*}
     &bot\ (5*5) \\
  =\ &\bot
\end{align*}
\end{multicols}
\caption{Normal order reduction}
\label{fig:normal-order}
\end{figure}

\paragraph{Normal order} This method of evaluation is the only method that
obeys the semantic property that \<\(\lambda\) \_ \to \(\bot\) \(\equiv \
\bot\)\>.  This is because normal order reduction will evaluate under a lambda
\tocite{Abramsky's lazy lambda calculus paper}.

Of the four, only the first three are commonly used as the basis for
programming languages. Most languages are call-by-value, this includes
functional languages such as Scheme, OCaml, SML, and Idris. Fewer languages
are call-by-name, Algol 60 being the most notable case. Scala, while being
call-by-value by default, does allow programmers to specify that some
functions use call-by-name. Lastly, call-by-need is used by Haskell, Clean,
Miranda, and our own F-lite.

\todoinline{Explain Church-Rosser, at least a little bit}

The reader may have noticed that in our examples above the result of evaluation
was always the same \emph{when they terminated}, regardless of evaluation
order.  This is an observation of a more general property about rewrite systems
known as \emph{confluence}. The lambda calculus was proven to be a confluent
system by Church and Rosser in 1936 \tocite{The original Church-Rosser paper}.
When discussing the lambda calculus specifically, it is referred to as the
Church-Rosser property.

\defineword{Church-Rosser Property}{The fact that the pure lambda calculus is
\emph{confluent} means that if there is more than one possible reduction step,
the choice of reduction does not alter the final result \emph{as long as the
chosen reduction steps eventually terminate}.}

We can illustrate the ramifications of the property with another simple
example.  \<const\> is a function that takes two arguments are returns the
first, \<inf\> is an infinite list of \<1\>s. 

\begin{haskell*}
    const x y = x \\
    &\quad&\hfill \\
    inf = 1 : inf
\end{haskell*}

The expression \<const ``NF'' inf\> has multiple reducible
expressions (redexes), but only one normal form (NF): \<``NF''\>.

\begin{figure}[!h]
\centering
\begin{multicols}{2}
\noindent
\begin{haskell*}
     && const ``NF'' inf \\
    &=& const ``NF'' (1:inf) \\
    &=& const ``NF'' (1:1:inf) \\
    &=& \(\dots\) \hscom{reduce forever}
\end{haskell*}
\begin{haskell*}
     && const ``NF'' inf \\
    &=& ``NF''
\end{haskell*}
\end{multicols}
\caption{Eager (left) and Lazy (right) evaluation order for $const$}
\label{fig:eagerandlazytake}
\end{figure}

The Church-Rosser property gives us a profound guarantee for our functional
programs: Given a valid expression, there is only one normal form for the
expression. This is true regardless of the order of reductions carried out
(given that the series of reductions actually terminates). So given a program,
there can be many possible reduction orders that all lead to the same result.
Additionally, if \emph{any} reduction order terminates, than call-by-need
evaluation terminates \citep{bird2014thinking}.  What does this mean for
sequential computation? For lazy languages, such as Haskell, this means that
there can only be non-termination if there would have been non-termination
under any other evaluation model!


With the Church-Rosser theorem in hand and knowing that call-by-need programs
are more likely to terminate than call-by-value equivalents, does the
evaluation order we choose affect our aims with regard to automatic
parallelisation? The main motivator for choosing call-by-need is that the
evaluation order makes purity\todo{define purity at some point} an essential
part of the language. Because evaluation of an expression can be delayed for
any amount of time, allowing arbitrary side-effects would make programming
extremely difficult. By keeping the language pure we gain the full benefits
of the Church-Rosser property.


Systems designed to take advantage of implicit parallelism
have been written for languages that use each of the three main evaluation
orders \tocite{We can cite Jens Nicolay here and some loop unrolling method
too}. We have decided on call-by-need semantics because it emphasises purity
and has the sharing of computation built into the execution model. The focus on
purity allows the compiler to take certain liberties with program
transformation that may not otherwise be valid \citep{jones1998transformation}.
In the case of auto-parallelisation, we are able to know that we could only
alter the semantics of a program by introducing non-termination. As we will see
in Chapter \ref{chap:discovery} there are methods to ensure we avoid this.

\paragraph{An aside} Many languages, including functional languages, that use
call-by-value semantics also provide the ability to perform arbitrary side
effects and mutation. This greatly hampers the feasibility of implicit
parallelisation because the \emph{sequence} of side-effects can alter the
semantics of the program. While programmers \emph{could} write pure programs
that do not rely on shared state, it is not enforced by the compiler as it is
for languages like Haskell. That being said there are techniques that can be
used to find safe parallelism in strict languages. \tocite{Matt Might's paper
at least}

\subsection{The Syntax and Semantics of F-lite}

Having motivated our choice of a lazy language we can now present F-lite
completely. We start with Figure \ref{fig:flite} where the abstract syntax
of F-lite is defined.

\begin{figure}
\centering
\begin{haskell*}
type Prog &=& [Decl] \\
      &\quad&\hfill \\
data Decl &=& Func Id [Pat] Exp \\
          &|& Data Id [Id] [(Id,[TypeExp])] \\
      &\quad&\hfill \\
type Id &=& String \\
      &\quad&\hfill \\
data Exp &=& App Exp [Exp] \\
         &|& Case Exp [Alt] \\
         &|& Let [Binding] Exp \\
         &|& Var Id \\
         &|& Con Id \\
         &|& Fun Id \\
         &|& Int Int \\
         &|& Lam [Id] Exp \\
         &|& Freeze Exp \\
         &|& Unfreeze Exp
\end{haskell*}
\caption{Abstract Syntax for F-lite}
\label{fig:flite}
\end{figure}

The language is an enriched lambda calculus with many of the usual
conveniences: case expressions, recursive lets, and user defined types. A key
point is the presence of expressions of the form \<Freeze e\> and \<Unfreeze e\>.
These expression are not found in the concrete syntax but are instead
added by the compiler to make the suspension and forcing of lazy values
explicit. We will see the utility of these expressions in Chapter
\ref{chap:discovery}.

\todoinline{Semantics!}
