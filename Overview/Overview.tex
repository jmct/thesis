As the majority of our work is in the form of a compiler it is important to
understand its organisation. Most of the phases present in our compiler can be
found in most standard compiler for lazy functional languages with the fact
that our compiler `iterates' by running the program and altering the
compilation based on that feedback. The compiler is organised into 8 main
phases, as follow:

\begin{enumerate}
    \item Parsing
    \item Defunctionalisation
    \item Projection-based Strictness Analysis
    \item Generation of strategies
    \item Placement of \verb-par- annotations
    \item $G$-Code Generation
    \item Execution
    \item Feedback and iteration
\end{enumerate}

Parsing is completely standard and we will therefore omit discussing that stage
of compilation. An interested reader is pointed towards ``Implementing
Functional Languages: A Tutorial'' \citep{PeytonJones:IFL}.


The rest of this dissertation focuses on the static analysis phases of the
compiler (defunctionalisation, strictness analysis, and the generation of
strategies) and the feedback and iteration phase. To see how the pieces
all fit together we will demonstrate the important stages of the compiler
with two small programs.

\subsection{Automatic Parallelisation: 1990's Style}

When research into implicit parallelism was at its height in the late 1980's and 1990's
much of the focus was on using static analysis to introduce parallelism to programs.
We can see how this was done with a simple example.

The program listed in Figure \ref{fig:tak} is the Tak program benchmark, often used
for testing the performance of recursion in interpreters and code generated by
compilers \citep{ExamplesOfRecursion}.

\begin{figure}
  \input{Blind/TakPhases/Tak.hs}
\caption{Source listing for Tak}
\label{fig:tak}
\end{figure}

Luckily, this program is already first-order, so we do not need to worry
about defunctionalisation. We therefore proceed directly to our strictness
analysis, this phase of the compiler is able to determine which function
arguments are \emph{definitely} needed by each function. In the case of
\verb|tak| the strictness analysis determines that all three arguments
are needed. \todoinline{show the strictness information}

After we perform our projection-based strictness analysis we can introduce the
safe \verb-par- annotations, transforming the program into a parallelised
version. The result of this transformation on Tak is listed in Figure
\ref{fig:takParred}.

\begin{figure}[!h]
  \input{Blind/TakPhases/TakParred.hs}
\caption{Source listing for Tak after analysis, transformation, and \texttt{par} placement}
\label{fig:takParred}
\end{figure}

Each needed \todo{should I define strict before this and use strict instead of
needed?} argument is given a name via a \verb-let- binding. This is so that any
parallel, or \verb-seq-ed, evaluation can be shared between threads. When there
are multiple strict arguments (as is the case for \verb-tak-) we spark the
arguments in left-to-right order except for the last strict argument, which we
\verb-seq-. This is a common technique that is used to avoid potential
collisions \citep{strategies}. Collisions occur when a thread requires the
result of another thread before the result has been evaluated. By ensuring that
one of the arguments is evaluated in the current thread (by using \texttt{seq})
we give the paralell threads more time to evaluate their arguments, lessening
the frequency of collisions.

Now that the parallelism has been introduced we can run our program on a multi-processor
machine and hope to get speedups.

\todoinline{Show results of sequential vs. parallel tak}

This is the hallmark of the conventional approach to automatic parallelism.
Determine a method of identifying potential parallelism, then transform the
program to exploit that parallelism. This approach however only addresses one
half of the criteria needed for taking advantage of implicit parallelism: where
is parallelism \emph{safe}?

While static analysis has determined that \verb-x'- and \verb-y'- can be
evaluated in parallel \emph{safely}, it does not determine whether parallel
evaluation of those expressions is \emph{worthwhile}. This is the crux of the
\emph{granularity problem}. In the case of Tak we can run the program and see
that we do indeed achieve performance increases from the parallelism that was
introduced\footnote{But it's not the fastest that we can achieve!} \todo{show
with charts!}. However there is no process for the compiler to alter its
decision of what \verb|par| annotations to introduce.

In the literature it was common to use further static analysis to estimate the
cost of evaluating an expression. This estimate would then be used to avoid
introducing parallelism that is too \emph{fine grained}. This was commonly done
with both heuristic oracles and/or using a static cost analysis
\citep{hogen1992automatic} \tocite{A cost analysis paper}.

% TODO: find a place for this, a discussion of collisions would be good.
%This is best explained through a simple example:
%
%\begin{figure}
%\begin{center}
%\begin{BVerbatim}
%fib :: Int -> Int
%fib 0 = 0
%fib 1 = 1
%fib n = let x = fib (n - 1)
%            y = fib (n - 2)
%        in par x (seq y (x + y))
%\end{BVerbatim}
%\end{center}
%\end{figure}
%
%There are two threads interacting during the execution of \verb-fib-: The
%current thread of execution, $P$ (for parent), and the thread sparked by
%\verb-par x-, $C$ (for child). If the \verb-seq- were a \verb-par-, or absent
%entirely, in \verb-fib- then there is the possibility that $P$ would require
%\verb-x- immediately when evaluating \verb-x + y-. This would cause $P$ to
%block on $C$, awaiting the completion of \verb-x-'s evaluation. By having the
%function use \verb-seq- we ensure that productive work was accomplished even if
%$P$ needs to wait for $C$'s completion.
%
%\hfill$\Box$

%The function \verb-tak- happens to be strict in all three of its arguments.
%Therefore we lift all three arguments into a \verb-let- binding, allowing their
%results to be shared. Then we arrange the parallel evaluation of each of the
%arguments using \verb-par- and \verb-seq- annotations. The \verb-seq-
%annotation is to ensure that \verb-z'- is evaluated in the current thread, so
%that the current thread does not immediately block waiting for \verb-x'- or
%\verb-y'-.
%


\subsection{Static Analysis is not Enough}

The propensity for researchers to prefer static analysis over dynamic technique
has many benefits, particularly the fact that they incur no runtime costs and
have many decades of work to build upon. However, in some cases the
(over)reliance on static analyses can be a hindrance instead of a benefit. We
argue that implicit parallelism is an area that requires compiler writers to
abandon the `static analysis only' methodology.

An intuitive argument for our view is that parallelising programs is something
that even \emph{experts} have difficulty with. The work of determining what
parallelism is worthwhile is often an \emph{iterative} process between the
programmer and the program. In other words: Programmers do not `write once',
but instead annotate a program, profile the program and alter the program
according to the results of profiling. We do we forbid compilers from utilising
the same process?

Program analysis does not have to pick sides, compilers can benefit (if the
implementors choose to) from \emph{both} static and dynamic analysis. In the
case of implicit parallelism this means having the compiler perform a form of
static analysis that provides an initial decision about parallelism in the
program, then having those decisions either confirmed or rejected by dynamic,
profile-driven analysis. There are several ways to achieve this goal, the
method we suggest is to utilise \emph{feedback-directed}
compilation\footnote{Sometimes referred to as feedback-directed optimisation or
iterative compilation}.

\defineword{Feedback-Directed Compilation}{The use of runtime profiles from
previous executions of the program in order to inform an aspect of compilation
or optimisation.}

This frees the compiler from having to answer all of the difficult questions
statically (and in one shot). Might and Prabhu suggest that the compiler has
two question it must ask when parallelising a sequential program \tocite{Might's stack reachability}:

\begin{enumerate}
    \item Where is parallelisation safe?
    \item Where is parallelisation beneficial?
\end{enumerate}

Compiler writers have many tools available when wanting to answer the first:
Strictness analysis, dependency analysis, control-flow analysis, etc.
\citep{hogen1992automatic, might2009interprocedural}. In fact, these analyses
are very good at answering the first question. The issue that has plagued work
in automatic parallelisation is that there is often \emph{too much} safe
parallelism\footnote{This is true for pure languages, in languages that allow
arbitrary side-effects finding safe parallelism can be more difficult.}.
Determining the answer to the second question then becomes essential.
Unfortunately statically determining the cost (or benefit) of evaluating an
expression in parallel is notoriously difficult to do statically
\citep{hogen1992automatic, might2009interprocedural}.


The problem is that while the safety of parallelising an expression is a static
property of the program, the benefit is also affected by extrinsic factors such
as the architecture the program is executed on. To be more precise: If there is
a minimum cost to create parallelism, static analysis can determine that some
expression are definitely \emph{not} worthwhile (by approximating the cost of
evaluating an expression `upwards'), however, static analysis cannot determine
that an expression is \emph{definitely} worthwhile. This is because the benefit
of parallelising an expression is a function of \emph{more} than the static
semantics of a program.

Let us consider simple thought experiment:

First consider an ideal parallel machine that has an infinite number of
processing units, zero-cost communication, and no context switches (each task
gets its own processor)\footnote{We did say \emph{ideal} machine.}. The only
cost to creating parallelism is the exact cost of the machine instructions used
to initialise the parallel task. If a task requires less time than the
initialisation cost, it is a net cost to the program and not worthwhile for
parallelism. While unrealistic, this scenario demonstrates that there are some
expressions that will not be worthwhile no matter the final program substrate
\todo{substrate?}.

Now consider the same machine except with a finite number of processing units.
Because each task is no longer able to retain a processor to itself, the machine
must schedule tasks, which imposes a context-switch cost on the tasks that
are interrupted. Now, whether a task is worthwhile depends not only on its
intrinsic cost, but on the effect it has on the rest of the computation by
interrupting other tasks. It is easy to see that more context-switches occur
as the number of processing units available is reduced.

The difficulty of predicting the benefit of a parallel task becomes even more
difficult as the machine becomes more realistic. Because of this we feel that
it is sensible to ask: ``Why have the compiler try at all?''. Instead of
having the compiler approximate this complex program property, we can run
the program itself and have the compiler `see' the effect its parallelisation
has on the final program.

One question remains: In what way do we combine the static analysis with the
feedback-directed compilation? There are several apparent approaches:

\begin{enumerate}
    \item Have the compiler introduce parallelism sparsely, and use the
        iteration to introduce more parallelism
    \item Have the compiler introduce what seems to be \emph{too much}
        parallelism and have the iteration remove some of the parallelism
    \item Have the compiler provide an initial `reasonable' parallelisation
        and have the iteration introduce/remove parallelism as necessary
\end{enumerate}

In choosing our approach we chose to play to the strengths of the language we
are conducting our experiments with. In pure functional languages there is an
abundance of safe parallelism. Historically, parallelism in pure lazy languages
often suffers more from the granularity problem than from a lack of available
parallelism \tocite{someone}. For this reason we have opted to use the second
approach: Use static analysis to introduce as much parallelism as possible and
use runtime feedback to disable some of the introduced parallelism.

We note that for a language that allows arbitrary side-effects it may be more
beneficial to use the first approach as the runtime feedback would aid in
determining which tasks were actually independent.

Later in Chapter \ref{chap:future} we will explore a variant of the third
approach.

\subsection{Automatic Parallelism in the 21st Century}

In order to address this issue we take advantage of
two key properties of our \verb-par- annotations:

\begin{enumerate}
    \item Each introduced \verb-par- sparks off a unique subexpression
            in the program's source
    \item The semantics of \verb-par- (as shown in Figure \ref{fig:seqandpar})
            allow us to return its second argument, ignoring the first,
            without changing the semantics of the program as a whole
\end{enumerate}

These two properties allow us to represent the \verb-par-s placed by static
analysis and transformation as a bit string. Each bit represents a specific
\verb-par- in the program's AST. When a \verb-par-'s bit is `on' the \verb-par-
behaves as normal, sparking off its first argument to be evaluated in parallel
and return its second argument. When the bit is `off' the \verb-par- returns
its second argument, ignoring the first.

This allows us to change the \emph{operational} behavior of the program without
altering any of the program's semantics. In other words, we are able to \emph{try}
evaluating subexpressions in parallel, without the risk of introducing behavior
that was not present in the original program.

% Below this is the overview from the IIP paper
% TODO: Merge overviews!
%\todo[inline]{Merge this overview with other overview}
%
%In this section we present the overall picture of our technique. Much of the
%discussion will center around the code presented in Figure \ref{sumLast}. In
%order to understand the code, it is useful to understand the architecture of the
%compiler.
%
%\todoinline{Canibalise some of the following for the overview above. Some of
%the content is good but it needs to be integrated with the above, right now
%it's just repetition with a few new things (how to read the code, introduce
%defunctionalisation, etc.)}

%\subsection{A Program Before Iteration}
%
%%\begin{figure}[t!]
%%  \input{Informed/SumPhases/SumEulerProcessed.core}
%%\caption{Core representation of \texttt{SumEuler} after defunctionalisation, demand
%%         analysis, and the introduction of initial \texttt{par} sites along
%%         with their associated strategies.(Auto-generated names have been
%%         replaced for better readability)}
%%\label{sumLast}
%%\end{figure}
%
%The code listed in Figure \ref{sumLast} is the resulting core representation
%of the program in Figure \ref{sumOrig} after our analysis and transformations.
%Specifically, the program has passed through compiler stages 1-5.
%
%Before we dive into the program itself, note the following points:
%
%\begin{itemize}
%    \item We only present the functions that have changed as a result of transformation
%    \item We have replaced auto-generated names with easier to read names
%    \item Functions ending in `S$N$' are derived strategies
%    \item Functions with an underscore in the name are the result of defunctionalisation
%\end{itemize}
%
%Taking a look at the program we can see several of the core ideas.
%
%\paragraph{Defunctionalisation}
%The application of \verb-map- to \verb-euler- has been replaced by a call to
%the specialised \verb-map_euler- function.  We no longer have the functions
%\verb-map- or \verb-filter- instead we have specialised versions of these
%functions (e.g. \verb-map_euler-)
%
%\paragraph{Introduction of parallelism}
%Two calls to \verb-par- have been introduced in the \texttt{main} function. Our
%work uses the traditional style for the parallel combinator \citep{strategies}
%
%\begin{verbatim}
%    par :: a -> b -> b
%    par x y = y
%\end{verbatim}
%
%The first argument is \emph{sparked} off to be evaluated in parallel and the
%function returns the second argument. This style is what allows us to easily
%\emph{switch off} a particular \verb-par- which causes that switched off
%\verb-par- to act like \verb-flip const-. This is explored further in
%\S\ref{sec:switchPar}.
%
%Each application of \verb-par- introduced by our compiler takes the following
%form: \verb-par (s x) e- where \verb-(s x)- is the application of derived
%strategy \verb-s- to a variable \verb$x$ and \verb$e$ is an expression containing \verb$x$ as a
%free variable.  In a \emph{top-level definition} like \verb-main-, \verb$x$ will be a
%name introduced by a \verb-let- expression. For \verb-par-s within the
%strategies themselves, \verb$x$ will be a name introduced by case analysis.
%
%\paragraph{Demand Analysis and Strategies}
%
%The compiler has introduced a number of strategies into the program. These
%strategies are derived based on the results of a demand analysis. A simple
%example from the program is the transformed version of \verb-length-. Because
%\verb-+- (for non-lazy integers) requires both arguments to be fully evaluated, it is safe to 
%evaluate the arguments to \verb-+- in parallel to the execution of its body.
%In order to benefit from the parallel evaluation, the structure must be shared.
%The introduction of the name \verb-len- accomplishes this. Because the type
%of \verb-len- is \verb-Int- evaluating the expression to WHNF is sufficient to
%evaluate the value fully. 
%
%Looking at the body of \verb-euler- where \verb-length- is called we see a
%similar pattern. In this case the demand analysis determines that it is
%safe the evaluate the \emph{spine} of the list passed to \verb-length-.
%The expression is given the name \verb-ys- and the strategy \verb-eulerListS1-
%is derived based on this information. Notice that the elements of the list
%passed to \verb-eulerListS1- are ignored in its body. 
%
%There are cases where there is a strict demand on an expression but we
%do not introduce parallel evaluation of the expression. This can be seen
%in the first argument to \verb-+- in the body of \verb-length-. The rules
%for which subexpressions are considered \emph{definitely} not worthwhile
%are discussed in \S\ref{sec:introPar}.

\paragraph{Iterative Improvement}

Just because an expression is \emph{able} to be evaluated in parallel does not
mean that doing so is beneficial! This is one of the critical problems in
implicit parallelism \citep{hogen1992automatic, hammond2000research,
Jones2009Tuning}. To combat this we run the program as presented in Figure
\ref{fig:takParred} and collect statistics about the amount of productive work
each \verb-par- is responsible for. The \verb-par-s that do not introduce a
worthwhile amount of parallelism (see discussion in Section \ref{sec:iterate})
are disabled, freeing the program from incurring the overhead of managing
threads for tasks with insufficient granularity\footnote{This can be seen as a
more extreme variation of Clack and Peyton Jones' ``Evaluate and die!'' model
of parallelism \citep{clack1986four}: Evaluate \emph{a lot} or die!}.

Luckily, the \verb-Tak- program has so few \verb-par-s that an exhaustive
exploration of the possible settings is easy. Table \ref{tab:takSettings}
shows the resulting speedup (as compared to the program with all parallelism
turned off) from the different possible \verb-par- settings. The bitstring
``$10$'' represents the version of \verb-Tak- where the \verb-par- labelled
$0$ in \ref{fig:takParred} is on and the \verb-par- labelled $1$ is off.

\begin{table}[h!]
\centering
\caption{Performance of \texttt{Tak} with different \texttt{par} settings}
\vspace{10pt}
\begin{tabular}{c || c c c c}
    \       & 00 & 01   & 10   & 11 \\
    \hline
    speedup & 1  & 1.56 & 0.43 & 0.97
\end{tabular}    
\label{tab:takSettings}
\end{table}

As we can see, more \verb-par-s is not necessarily better! In fact, for our
interpreter the setting $10$ results in quite a lot of thread collisions,
meaning that a thread pays the overhead for its paralellism and is almost
immediately blocked.  Paying the cost of creating and managing the parallelism,
but not being able to perform an actual work is why the program performs
\emph{worse} than the version of \verb-Tak- with all the parallelism switched
off.

So why introduce this parallelism in the first place? Because the granularity
and possible interference of parallel threads is difficult to know
\emph{statically} at compile time. Not only is it difficult to know these
properties statically, but these properties differ from architecture to
architecture \citep{SteuwerPortable}! If we err on the side of generosity with
our \verb-par- annotations we can then use \emph{runtime} profiling to gather
information about the granularity and interference of threads. 


\section{Summary}

This chapter provided a high-level overview of our technique for exploiting the
parallelism that is inherent in functional programs. We motivated our choice of
a \emph{lazy} language (normal-order reduction using call-by-need) by pointing
out its emphasis on purity and the fact that the sharing of values is built-in
to the evaluation model.

We introduced the structure of our compiler and walked through the steps of our
technique using the \verb-Tak- program as a small example. This allows for a
clearer notion of where the remaining chapters fit into our technique and
provides a common vocabulary for the rest of the thesis.

\paragraph{}

Now that we have presented the high-level view of our work we will now explore
each of the stages in depth. Chapter \ref{chap:discovery} introduces strictness
analysis and discusses the history of its development, and why we have chosen
\emph{projection-based} strictness analysis for our compiler. Chapter
\ref{chap:derivation} presents how to use the results from projection-based
strictness analysis to \emph{automatically} derive parallel strategies that are
then used to introduce parallelism into the source program. Chapters \ref{chap:blind}
and \ref{chap:prof-search} represent the iterative phase of our compiler, discussing
two methods of searching the space of \verb-par- settings.
